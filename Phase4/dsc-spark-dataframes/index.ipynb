{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Spark DataFrames\n", "\n", "## Introduction\n", "\n", "You've now explored how to perform operations on Spark RDDs for simple MapReduce tasks. This is useful for contexts where the low-level Unstructured API is most appropriate, but now we're going to move on to using a more intuitive and powerful interface: Spark DataFrames!\n", "\n", "\n", "## Objectives\n", "\n", "You will be able to: \n", "\n", "- Load and manipulate data using Spark SQL DataFrames\n", "- Describe the similarities and differences between RDDs, Spark SQL DataFrames, and pandas DataFrames"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Spark SQL DataFrames\n", "\n", "From the [Spark SQL docs](https://spark.apache.org/docs/latest/sql-programming-guide.html):\n", "\n", "> Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed.\n", "\n", "Spark SQL has both a SQL interface and a DataFrame interface. We will primarily use the DataFrame interface but it's useful to be aware of both.\n", "\n", "### Understanding SparkSession\n", "\n", "In the previous lessons, we were using the Unstructured API and therefore we connected to Spark using a SparkContext. Here, we will be using SparkSession instead ([documentation here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)), which is actually wrapped around a SparkContext under the hood. SparkSession is designed for interacting with high-level Spark SQL data structures (e.g. DataFrames) whereas SparkContext is design with interacting with low-level Spark Core data structures (e.g. RDDs).\n", "\n", "A SparkSession is created using a *builder* pattern ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.html)) and its conventional name is `spark`:"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import SparkSession\n", "spark = SparkSession.builder.master('local').getOrCreate()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Creating a Spark SQL DataFrame with PySpark\n", "\n", "Now that we have a SparkSession, we can create a DataFrame using the `createDataFrame` method ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html))! One way to do this would just be to hard-code the data using built-in Python types:"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": ["DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df = spark.createDataFrame([\n", "    {\"color\": \"red\", \"number\": 4, \"count\": 1, \"valid\": True},\n", "    {\"color\": \"blue\", \"number\": 7, \"count\": 2, \"valid\": False},\n", "    {\"color\": \"green\", \"number\": 1, \"count\": 3, \"valid\": True}\n", "])\n", "spark_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When the notebook displays `spark_df`, it is showing the schema between the square brackets -- in other words, it is displaying the column names and data types. We did not specify explicit data types for the columns, so Spark inferred them for us.\n", "\n", "You can view a nice print-out of the schema like this:"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- color: string (nullable = true)\n", " |-- count: long (nullable = true)\n", " |-- number: long (nullable = true)\n", " |-- valid: boolean (nullable = true)\n", "\n"]}], "source": ["spark_df.printSchema()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Basic Features of Spark SQL DataFrames\n", "\n", "#### RDD Methods\n", "\n", "Many of the familiar RDD methods will work with DataFrames.\n", "\n", "For example, `.collect()` to load all of the data:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": ["[Row(color='red', count=1, number=4, valid=True),\n", " Row(color='blue', count=2, number=7, valid=False),\n", " Row(color='green', count=3, number=1, valid=True)]"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.collect()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And `.take(n)` to return `n` rows of data: "]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["[Row(color='red', count=1, number=4, valid=True),\n", " Row(color='blue', count=2, number=7, valid=False)]"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.take(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### DataFrame-Specific Methods\n", "\n", "In addition to the methods that work on RDDs, there are methods specific to DataFrames.\n", "\n", "For example, we can view all of the data in a tabular format using `.show()`:"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-----+-----+------+-----+\n", "|color|count|number|valid|\n", "+-----+-----+------+-----+\n", "|  red|    1|     4| true|\n", "| blue|    2|     7|false|\n", "|green|    3|     1| true|\n", "+-----+-----+------+-----+\n", "\n"]}], "source": ["spark_df.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(We will add a `.show()` to the end of most of the following examples, since it's easier to read that way.)\n", "\n", "If we want to select the data from one or more specific columns, we can use `.select()`:"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+------+\n", "|number|\n", "+------+\n", "|     4|\n", "|     7|\n", "|     1|\n", "+------+\n", "\n"]}], "source": ["spark_df.select(\"number\").show()"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+------+-----+\n", "|number|valid|\n", "+------+-----+\n", "|     4| true|\n", "|     7|false|\n", "|     1| true|\n", "+------+-----+\n", "\n"]}], "source": ["spark_df.select([\"number\", \"valid\"]).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Familiar Techniques for Pandas Developers\n", "\n", "There are also several attributes and methods that work very similarly for Spark SQL DataFrames as they do with pandas DataFrames:"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["['color', 'count', 'number', 'valid']"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.columns"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-------+-----+-----+------+\n", "|summary|color|count|number|\n", "+-------+-----+-----+------+\n", "|  count|    3|    3|     3|\n", "|   mean| null|  2.0|   4.0|\n", "| stddev| null|  1.0|   3.0|\n", "|    min| blue|    1|     1|\n", "|    max|  red|    3|     7|\n", "+-------+-----+-----+------+\n", "\n"]}], "source": ["spark_df.describe().show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Loading the Forest Fire Dataset\n", "\n", "For this example, we're going to be using the [Forest Fire dataset](https://archive.ics.uci.edu/ml/datasets/Forest+Fires) from UCI, which contains data about the area burned by wildfires in the Northeast region of Portugal in relation to numerous other factors.\n", " \n", "We'll use `spark.read.csv` to load in the dataset: "]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "|  X|  Y|month|day|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|\n", "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "|  7|  5|  mar|fri|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|\n", "|  7|  4|  oct|tue|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|\n", "|  7|  4|  oct|sat|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|\n", "|  8|  6|  mar|fri|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|\n", "|  8|  6|  mar|sun|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|\n", "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "only showing top 5 rows\n", "\n"]}], "source": ["fire_df = spark.read.csv(\"forestfires.csv\", header=\"true\", inferSchema=\"true\")\n", "fire_df.show(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Spark DataFrame Aggregations\n", "\n", "Let's investigate to see if there is any relationship between what month it is and the area of fire.\n", "\n", "First we'll group by the `month` column, then aggregate based on the mean of the `area` column for that group:"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": ["DataFrame[month: string, avg(area): double]"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["fire_df_months = fire_df.groupBy('month').agg({'area': 'mean'})\n", "fire_df_months"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice how the grouped DataFrame is not returned when you call the aggregation method. Remember, this is still Spark! The transformations and actions are kept separate so that it is easier to manage large quantities of data. You can perform the transformation by calling `.collect()`: "]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": ["[Row(month='jun', avg(area)=5.841176470588234),\n", " Row(month='aug', avg(area)=12.489076086956521),\n", " Row(month='may', avg(area)=19.24),\n", " Row(month='feb', avg(area)=6.275),\n", " Row(month='sep', avg(area)=17.942616279069753),\n", " Row(month='mar', avg(area)=4.356666666666667),\n", " Row(month='oct', avg(area)=6.638),\n", " Row(month='jul', avg(area)=14.3696875),\n", " Row(month='nov', avg(area)=0.0),\n", " Row(month='apr', avg(area)=8.891111111111112),\n", " Row(month='dec', avg(area)=13.33),\n", " Row(month='jan', avg(area)=0.0)]"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["fire_df_months.collect()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's show that as a table instead, and also order by the average area of fire:"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-----+------------------+\n", "|month|         avg(area)|\n", "+-----+------------------+\n", "|  jan|               0.0|\n", "|  nov|               0.0|\n", "|  mar| 4.356666666666667|\n", "|  jun| 5.841176470588234|\n", "|  feb|             6.275|\n", "|  oct|             6.638|\n", "|  apr| 8.891111111111112|\n", "|  aug|12.489076086956521|\n", "|  dec|             13.33|\n", "|  jul|        14.3696875|\n", "|  sep|17.942616279069753|\n", "|  may|             19.24|\n", "+-----+------------------+\n", "\n"]}], "source": ["fire_df_months.orderBy(\"avg(area)\").show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, there seem to be larger area fires during what would be considered the summer months in Portugal. On your own, practice more aggregations and manipulations that you might be able to perform on this dataset. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Boolean Masking \n", "\n", "Boolean masking also works with PySpark DataFrames just like Pandas DataFrames, the only difference being that the `.filter()` method is used in PySpark. To try this out, let's compare the amount of fire in those areas with absolutely no rain to those areas that had rain."]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["no_rain = fire_df.filter(fire_df['rain'] == 0.0)\n", "some_rain = fire_df.filter(fire_df['rain'] > 0.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, to perform calculations to find the mean of a column (without aggregating first), we'll have to import functions from `pyspark.sql`. As always, to read more about them, check out the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)."]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["no rain fire area:\n", "+------------------+\n", "|         avg(area)|\n", "+------------------+\n", "|13.023693516699408|\n", "+------------------+\n", "\n", "some rain fire area:\n", "+---------+\n", "|avg(area)|\n", "+---------+\n", "|  1.62375|\n", "+---------+\n", "\n"]}], "source": ["from pyspark.sql.functions import mean\n", "\n", "print('no rain fire area:')\n", "no_rain.select(mean('area')).show()\n", "\n", "print('some rain fire area:')\n", "some_rain.select(mean('area')).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Yes there's definitely something there! Unsurprisingly, rain plays in a big factor in the spread of wildfire.\n", "\n", "Let's obtain data from only the summer months in Portugal (June, July, and August). We can also do the same for the winter months in Portugal (December, January, February)."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["summer months fire area:\n", "+------------------+\n", "|         avg(area)|\n", "+------------------+\n", "|12.262317596566525|\n", "+------------------+\n", "\n", "winter months fire area\n", "+-----------------+\n", "|        avg(area)|\n", "+-----------------+\n", "|7.918387096774193|\n", "+-----------------+\n", "\n"]}], "source": ["summer_months = fire_df.filter(fire_df['month'].isin(['jun','jul','aug']))\n", "winter_months = fire_df.filter(fire_df['month'].isin(['dec','jan','feb']))\n", "\n", "print('summer months fire area:')\n", "summer_months.select(mean('area')).show()\n", "\n", "print('winter months fire area')\n", "winter_months.select(mean('area')).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparison between DataFrames\n", "\n", "Although Spark SQL DataFrames and pandas DataFrames have some features in common, they are not the same. We'll demonstrate some similarities and differences below.\n", "\n", "First, we'll create a `pandas_df` with the same data as `spark_df` and compare the two:"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "pandas_df = pd.DataFrame([\n", "    {\"color\": \"red\", \"number\": 4, \"count\": 1, \"valid\": True},\n", "    {\"color\": \"blue\", \"number\": 7, \"count\": 2, \"valid\": False},\n", "    {\"color\": \"green\", \"number\": 1, \"count\": 3, \"valid\": True}\n", "])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Displaying Data"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>color</th>\n", "      <th>number</th>\n", "      <th>count</th>\n", "      <th>valid</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>red</td>\n", "      <td>4</td>\n", "      <td>1</td>\n", "      <td>True</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>blue</td>\n", "      <td>7</td>\n", "      <td>2</td>\n", "      <td>False</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>green</td>\n", "      <td>1</td>\n", "      <td>3</td>\n", "      <td>True</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   color  number  count  valid\n", "0    red       4      1   True\n", "1   blue       7      2  False\n", "2  green       1      3   True"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"data": {"text/plain": ["DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"]}, "execution_count": 20, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["One difference you'll notice immediately, which has been pointed out a couple times in this lesson already, is that a Spark DataFrame loads lazily but a pandas DataFrame does not. Therefore if we just type the name of the variable we see at least a preview of the pandas data, but no preview of the Spark data.\n", "\n", "### Attributes"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"data": {"text/plain": ["color     object\n", "number     int64\n", "count      int64\n", "valid       bool\n", "dtype: object"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df.dtypes"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": ["[('color', 'string'),\n", " ('count', 'bigint'),\n", " ('number', 'bigint'),\n", " ('valid', 'boolean')]"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.dtypes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sometimes a Spark DataFrame will have an attribute with a familiar name, but it won't work quite the same way. For example, the `dtypes` attribute is present for both, but in pandas it returns a Series and in Spark SQL it returns a list of tuples.\n", "\n", "The actual data types listed are also different, although they correspond to each other. For example, the `object` data type in pandas corresponds to the `string` data type in Spark.\n", "\n", "### Methods\n", "\n", "As mentioned previously, Spark SQL DataFrames have methods related to their underlying RDD data structures. Pandas DataFrames do not have these methods.\n", "\n", "Even when the methods have the same name, they might not behave the same way:"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>number</th>\n", "      <th>count</th>\n", "      <th>valid</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>number</th>\n", "      <td>1.000000</td>\n", "      <td>-0.5</td>\n", "      <td>-0.866025</td>\n", "    </tr>\n", "    <tr>\n", "      <th>count</th>\n", "      <td>-0.500000</td>\n", "      <td>1.0</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>valid</th>\n", "      <td>-0.866025</td>\n", "      <td>0.0</td>\n", "      <td>1.000000</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["          number  count     valid\n", "number  1.000000   -0.5 -0.866025\n", "count  -0.500000    1.0  0.000000\n", "valid  -0.866025    0.0  1.000000"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df.corr()"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"data": {"text/plain": ["-0.5"]}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.corr(\"number\", \"count\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the example above, both have a `corr` method that is used for computing correlations between columns, but they work fairly differently.\n", "\n", "* The pandas method does not require any arguments and returns an entire DataFrame showing the correlations between all numeric variables (including `valid`, which contains booleans). \n", "* The Spark SQL method requires that you specify two column names and returns a single floating point number indicating the correlation between those two columns.\n", "\n", "Watch out for distinctions like this! And don't hesitate to read through the [Spark SQL documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#spark-sql) when you're trying out a new method.\n", "\n", "### Selecting Columns and Boolean Masking\n", "\n", "In pandas, you can select the data in a single column and it will be viewable as a Series:"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"data": {"text/plain": ["0      red\n", "1     blue\n", "2    green\n", "Name: color, dtype: object"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df[\"color\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can apply a method to that Series to get back a Series of booleans:"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"data": {"text/plain": ["0     True\n", "1    False\n", "2     True\n", "Name: color, dtype: bool"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df[\"color\"].str.contains(\"r\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And then we can also use that Series of booleans to filter the DataFrame using boolean masking:"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>color</th>\n", "      <th>number</th>\n", "      <th>count</th>\n", "      <th>valid</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>red</td>\n", "      <td>4</td>\n", "      <td>1</td>\n", "      <td>True</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>green</td>\n", "      <td>1</td>\n", "      <td>3</td>\n", "      <td>True</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   color  number  count  valid\n", "0    red       4      1   True\n", "2  green       1      3   True"]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df[pandas_df[\"color\"].str.contains(\"r\")]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In Spark, the intermediate Column data is not viewable and is only useful for applying the boolean mask.\n", "\n", "First we select the data in the `color` column, but if we try to `show()` the result, we get an error:"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"data": {"text/plain": ["Column<'color'>"]}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df[\"color\"]"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'TypeError'>\n", "'Column' object is not callable\n"]}], "source": ["try:\n", "    spark_df[\"color\"].show()\n", "except Exception as e:\n", "    print(type(e))\n", "    print(e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can chain a `contains` method call onto the Column, but again, if we try to `show()` it, we get an error:"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"data": {"text/plain": ["Column<'contains(color, r)'>"]}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df[\"color\"].contains(\"r\")"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'TypeError'>\n", "'Column' object is not callable\n"]}], "source": ["try:\n", "    spark_df[\"color\"].contains(\"r\").show()\n", "except Exception as e:\n", "    print(type(e))\n", "    print(e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we want data we can show using this boolean mask, we have to apply the `filter` method:"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"data": {"text/plain": ["DataFrame[color: string, count: bigint, number: bigint, valid: boolean]"]}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": ["spark_df.filter(spark_df[\"color\"].contains(\"r\"))"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+-----+-----+------+-----+\n", "|color|count|number|valid|\n", "+-----+-----+------+-----+\n", "|  red|    1|     4| true|\n", "|green|    3|     1| true|\n", "+-----+-----+------+-----+\n", "\n"]}], "source": ["spark_df.filter(spark_df[\"color\"].contains(\"r\")).show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Converting Between DataFrame Types\n", "\n", "In general, it is not particularly efficient to convert from one DataFrame type to another. Usually the reason that you are using a Spark SQL DataFrame is that you are working with Big Data and require computational optimization, so it wouldn't make much sense to convert it to a pandas DataFrame.\n", "\n", "However you can imagine some specific circumstances where you might want to use pandas for debugging, visualization, or some other task that just isn't working in Spark SQL. If you need to do that, check out [this documentation](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Stop the SparkSession"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["spark.stop()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "In this lesson, you explored Spark SQL DataFrames and their methods for displaying and manipulating data. You created DataFrames from base Python data structures as well as from a CSV file, and performed selection, filtering, and aggregation tasks along the way. Finally, you learned about the similarities and differences between Spark SQL DataFrames, their underlying RDDs, and pandas DataFrames."]}], "metadata": {"kernelspec": {"display_name": "Python (python3)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.5"}}, "nbformat": 4, "nbformat_minor": 2}