{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3131682",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "Now that we've run the model locally with one month of data, we'd like to build the model using multiple months. The total data *zipped* is about ~10GB, but unzipped it will be much more. We can serialize the data to a Pandas dataframe but most likely it will throw memory issues depending on the machine you have. We want to write code for one month, locally, using PySpark then migrate the code to run on EMR, and take multiple unzipped files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c4952",
   "metadata": {},
   "source": [
    "First, use the boto3 client to set up the s3 resource then check if the file exists in your bucket. If it doesn't exist, you might have to upload it. You can skip this step for now, but will be helpful for the next lab, where you'll be pulling the data from the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8849d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark import SparkSession\n",
    "#solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"XGBoost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf35016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path could be local or boto3\n",
    "path = \"\"\n",
    "df = spark.read.csv(path=path, header=\"true\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c0a8c",
   "metadata": {},
   "source": [
    "### How many unique customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d015282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab0c8f",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "\n",
    "Using the logic from the previous lab, use pyspark functions to explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb4ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176608c0",
   "metadata": {},
   "source": [
    "# Modeling: Cart Abandonment\n",
    "\n",
    "The model will be similar - let's build out the new features then start building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6375f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution for additional columns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b196044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df should be the dataframe with additional columns/features\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 42)\n",
    "print(\"There are %d training examples and %d test examples.\" % (train.count(), test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abe72a",
   "metadata": {},
   "source": [
    "Most MLlib algorithms require a single input column containing a vector of features and a single target column. The DataFrame currently has one column for each feature. MLlib provides functions to help you prepare the dataset in the required format.\n",
    "\n",
    "MLlib pipelines combine multiple steps into a single workflow, making it easier to iterate as you develop the model.\n",
    "\n",
    "In this example, you create a pipeline using the following functions:\n",
    "\n",
    "- VectorAssembler: Assembles the feature columns into a feature vector.\n",
    "- VectorIndexer: Identifies columns that should be treated as categorical. This is done heuristically, identifying any column with a small number of distinct values as categorical. In this example, the cart abandonment feature would be categorical (0 or 1)\n",
    "- XgboostRegressor: Uses the XgboostRegressor estimator to learn how to predict rental counts from the feature vectors.\n",
    "- CrossValidator: The XGBoost regression algorithm has several hyperparameters. This notebook illustrates how to use hyperparameter tuning in Spark. This capability automatically tests a grid of hyperparameters and chooses the best resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0271bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    " \n",
    "# Remove the target column from the input feature set.\n",
    "featuresCols = df.columns\n",
    "# featuresCols.remove('your target column')\n",
    " \n",
    "# vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    " \n",
    "# vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15958968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl.xgboost import XgboostRegressor\n",
    " \n",
    "xgb_regressor = XgboostRegressor(num_workers=3, labelCol=\"your_label_column\", missing=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02935ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    " \n",
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: maximum depth of each decision tree \n",
    "#  - maxIter: iterations, or the total number of trees \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(xgb_regressor.max_depth, [2, 5])\\\n",
    "  .addGrid(xgb_regressor.n_estimators, [10, 100])\\\n",
    "  .build()\n",
    " \n",
    "# Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=xgb_regressor.getLabelCol(),\n",
    "                                predictionCol=xgb_regressor.getPredictionCol())\n",
    " \n",
    "# Declare the CrossValidator, which performs the model tuning.\n",
    "cv = CrossValidator(estimator=xgb_regressor, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70425e0",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b24c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
