{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Tuning Neural Networks with Regularization - Lab \n", "\n", "## Introduction\n", "\n", "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "- Apply early stopping criteria with a neural network \n", "- Apply L1, L2, and dropout regularization on a neural network  \n", "- Examine the effects of training with more data on a neural network  \n", "\n", "\n", "## Load the Data\n", "\n", "Run the following cell to import some of the libraries and classes you'll need in this lab. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import random\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from sklearn.model_selection import train_test_split\n", "from keras.utils.np_utils import to_categorical\n", "from sklearn.preprocessing import LabelBinarizer\n", "from keras.preprocessing.text import Tokenizer\n", "\n", "import warnings\n", "warnings.filterwarnings(action='ignore', category=FutureWarning)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load and preview the dataset\n", "df = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing Overview\n", "\n", "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n", "\n", "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n", "* Train - test split\n", "* One-hot encoding your complaint text\n", "* Transforming your category labels \n", "\n", "## Preprocessing: Generate a Random Sample\n", "\n", "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n", "\n", "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n", "- Split this sample into `X` and `y` "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Downsample the data\n", "df_sample = None\n", "\n", "# Split the data into X and y\n", "y = None\n", "X = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train-test split\n", "\n", "- Split the data into training and test sets \n", "- Assign 1500 obervations to the test set and use 42 as the seed "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Split data into training and test sets\n", "X_train, X_test, y_train, y_test = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Validation set \n", "\n", "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n", "\n", "Run the cell below to further divide the training data into training and validation sets. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Split the data into training and validation sets\n", "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing: One-hot Encoding the Complaints\n", "\n", "As before, you need to do some preprocessing before building a neural network model. \n", "\n", "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n", "- Transform the training, validate, and test sets "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use one-hot encoding to reformat the complaints into a matrix of vectors \n", "# Only keep the 2000 most common words \n", "\n", "tokenizer = None\n", "\n", "\n", "X_train_tokens = None\n", "X_val_tokens = None\n", "X_test_tokens = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing: Encoding the Products\n", "\n", "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n", "  \n", "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n", "\n", "Transform the training, validate, and test sets. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Transform the product labels to numerical values\n", "lb = None\n", "\n", "\n", "y_train_lb = None\n", "y_val_lb = None\n", "y_test_lb = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A Baseline Model \n", "\n", "Rebuild a fully connected (Dense) layer network:  \n", "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n", "- Use a `'softmax'` activation function for the output layer  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build a baseline neural network model using Keras\n", "random.seed(123)\n", "from keras import models\n", "from keras import layers\n", "baseline_model = None\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Compile the Model\n", "\n", "Compile this model with: \n", "\n", "- a stochastic gradient descent optimizer \n", "- `'categorical_crossentropy'` as the loss function \n", "- a focus on `'accuracy'` "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compile the model\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the Model\n", "\n", "- Train the model for 150 epochs in mini-batches of 256 samples \n", "- Include the `validation_data` argument to ensure you keep track of the validation loss  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Train the model\n", "baseline_model_val = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Model Performance\n", "\n", "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Access the history attribute and store the dictionary\n", "baseline_model_val_dict = None\n", "\n", "# Print the keys\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate this model on the training data: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = None\n", "print('----------')\n", "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate this model on the test data: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test = None\n", "print('----------')\n", "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot the Results \n", "\n", "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Loss vs number of epochs with train and validation sets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a second plot comparing training and validation accuracy to the number of epochs. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Accuracy vs number of epochs with train and validation sets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n", "\n", "\n", "## Early Stopping\n", "\n", "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n", "\n", "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random.seed(123)\n", "model_2 = models.Sequential()\n", "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n", "model_2.add(layers.Dense(25, activation='relu'))\n", "model_2.add(layers.Dense(7, activation='softmax'))\n", "\n", "model_2.compile(optimizer='SGD', \n", "                loss='categorical_crossentropy', \n", "                metrics=['acc'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n", "- Define a list, `early_stopping`: \n", "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n", "  - Save the best model while monitoring `'val_loss'` \n", " \n", "> If you need help, consult [documentation](https://keras.io/callbacks/).   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import EarlyStopping and ModelCheckpoint\n", "\n", "\n", "# Define the callbacks\n", "early_stopping = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_2_val = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the best (saved) model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load the best (saved) model\n", "\n", "saved_model = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, use this model to to calculate the training and test accuracy: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n", "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n", "\n", "print('----------')\n", "\n", "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n", "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n", "\n", "Now, take a look at how regularization techniques can further improve your model performance. \n", "\n", "## L2 Regularization \n", "\n", "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n", "\n", "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n", "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import regularizers\n", "\n", "random.seed(123)\n", "L2_model = models.Sequential()\n", "\n", "# Add the input and first hidden layer\n", "\n", "\n", "# Add another hidden layer\n", "\n", "\n", "# Add an output layer\n", "L2_model.add(layers.Dense(7, activation='softmax'))\n", "\n", "# Compile the model\n", "L2_model.compile(optimizer='SGD', \n", "                 loss='categorical_crossentropy', \n", "                 metrics=['acc'])\n", "\n", "# Train the model \n", "L2_model_val = L2_model.fit(X_train_tokens, \n", "                            y_train_lb, \n", "                            epochs=150, \n", "                            batch_size=256, \n", "                            validation_data=(X_val_tokens, y_val_lb))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# L2 model details\n", "L2_model_dict = L2_model_val.history\n", "L2_acc_values = L2_model_dict['acc'] \n", "L2_val_acc_values = L2_model_dict['val_acc']\n", "\n", "# Baseline model\n", "baseline_model_acc = baseline_model_val_dict['acc'] \n", "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n", "\n", "# Plot the accuracy for these models\n", "fig, ax = plt.subplots(figsize=(12, 8))\n", "epochs = range(1, len(acc_values) + 1)\n", "ax.plot(epochs, L2_acc_values, label='Training acc L2')\n", "ax.plot(epochs, L2_val_acc_values, label='Validation acc L2')\n", "ax.plot(epochs, baseline_model_acc, label='Training acc')\n", "ax.plot(epochs, baseline_model_val_acc, label='Validation acc')\n", "ax.set_title('Training & validation accuracy L2 vs regular')\n", "ax.set_xlabel('Epochs')\n", "ax.set_ylabel('Accuracy')\n", "ax.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n", "\n", "\n", "## L1 Regularization\n", "\n", "Now have a look at L1 regularization. Will this work better? \n", "\n", "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n", "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random.seed(123)\n", "L1_model = models.Sequential()\n", "\n", "# Add the input and first hidden layer\n", "\n", "\n", "# Add a hidden layer\n", "\n", "\n", "# Add an output layer\n", "L1_model.add(layers.Dense(7, activation='softmax'))\n", "\n", "# Compile the model\n", "L1_model.compile(optimizer='SGD', \n", "                 loss='categorical_crossentropy', \n", "                 metrics=['acc'])\n", "\n", "# Train the model \n", "L1_model_val = L1_model.fit(X_train_tokens, \n", "                            y_train_lb, \n", "                            epochs=150, \n", "                            batch_size=256, \n", "                            validation_data=(X_val_tokens, y_val_lb))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the training as well as the validation accuracy for the L1 model: "]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(12, 8))\n", "\n", "L1_model_dict = L1_model_val.history\n", "\n", "acc_values = L1_model_dict['acc'] \n", "val_acc_values = L1_model_dict['val_acc']\n", "\n", "epochs = range(1, len(acc_values) + 1)\n", "ax.plot(epochs, acc_values, label='Training acc L1')\n", "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n", "ax.set_title('Training & validation accuracy with L1 regularization')\n", "ax.set_xlabel('Epochs')\n", "ax.set_ylabel('Accuracy')\n", "ax.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n", "\n", "\n", "## Dropout Regularization \n", "\n", "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n", "\n", "- Apply a dropout rate of 30% to the input layer \n", "- Add a first hidden layer with 50 units and `'relu'` activation \n", "- Apply a dropout rate of 30% to the first hidden layer \n", "- Add a second hidden layer with 25 units and `'relu'` activation \n", "- Apply a dropout rate of 30% to the second hidden layer \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u23f0 This cell may take about a minute to run\n", "random.seed(123)\n", "dropout_model = models.Sequential()\n", "\n", "# Implement dropout to the input layer\n", "# NOTE: This is where you define the number of units in the input layer\n", "\n", "\n", "# Add the first hidden layer\n", "\n", "\n", "# Implement dropout to the first hidden layer \n", "\n", "\n", "# Add the second hidden layer\n", "\n", "\n", "# Implement dropout to the second hidden layer \n", "\n", "\n", "# Add the output layer\n", "dropout_model.add(layers.Dense(7, activation='softmax'))\n", "\n", "\n", "# Compile the model\n", "dropout_model.compile(optimizer='SGD', \n", "                      loss='categorical_crossentropy', \n", "                      metrics=['acc'])\n", "\n", "# Train the model\n", "dropout_model_val = dropout_model.fit(X_train_tokens, \n", "                                      y_train_lb, \n", "                                      epochs=150, \n", "                                      batch_size=256, \n", "                                      validation_data=(X_val_tokens, y_val_lb))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["results_train = model.evaluate(X_train_tokens, y_train_lb)\n", "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n", "\n", "print('----------')\n", "\n", "results_test = model.evaluate(X_test_tokens, y_test_lb)\n", "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n", "\n", "## Bigger Data? \n", "\n", "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_bigger_sample = df.sample(40000, random_state=123)\n", "\n", "X = df['Consumer complaint narrative']\n", "y = df['Product']\n", "\n", "# Train-test split\n", "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n", "                                                                                y, \n", "                                                                                test_size=6000, \n", "                                                                                random_state=42)\n", "\n", "# Validation set\n", "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n", "                                                                                          y_train_bigger, \n", "                                                                                          test_size=4000, \n", "                                                                                          random_state=42)\n", "\n", "\n", "# One-hot encoding of the complaints\n", "tokenizer = Tokenizer(num_words=2000)\n", "tokenizer.fit_on_texts(X_train_final_bigger)\n", "\n", "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n", "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n", "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n", "\n", "# One-hot encoding of products\n", "lb = LabelBinarizer()\n", "lb.fit(y_train_final_bigger)\n", "\n", "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n", "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n", "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u23f0 This cell may take several minutes to run\n", "random.seed(123)\n", "bigger_data_model = models.Sequential()\n", "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n", "bigger_data_model.add(layers.Dense(25, activation='relu'))\n", "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n", "\n", "bigger_data_model.compile(optimizer='SGD', \n", "                          loss='categorical_crossentropy', \n", "                          metrics=['acc'])\n", "\n", "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n", "                                              y_train_lb_bigger,  \n", "                                              epochs=150,  \n", "                                              batch_size=256,  \n", "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n", "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n", "\n", "print('----------')\n", "\n", "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n", "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n", "\n", "\n", "## Additional Resources\n", "\n", "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n", "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n", "* https://catalog.data.gov/dataset/consumer-complaint-database \n", "\n", "\n", "## Summary  \n", "\n", "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}