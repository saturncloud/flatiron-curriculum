{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Machine Learning with Spark\n", "\n", "## Introduction\n", "\n", "Now that we've performed some data manipulation and aggregation with Spark SQL DataFrames, let's get to the really cool stuff: machine learning!\n", "\n", "## Objectives\n", "\n", "You will be able to: \n", "\n", "- Define estimators and transformers in Spark ML \n", "- Create a Spark ML pipeline that transforms data and runs over a grid of hyperparameters \n", "\n", "## A Tale of Two Libraries\n", "\n", "If you look at the PySpark documentation, you'll notice that there are two different libraries for machine learning, [MLlib (RDD-based)](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html) and [MLlib (DataFrame-based)](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html).\n", "\n", "These libraries are extremely similar to one another, the only difference being the kinds of data structures they are designed to operate on. The Spark maintainers have announced that the RDD-based library is now in [\"maintenance mode\"](https://spark.apache.org/docs/latest/ml-guide.html) and that the DataFrame-based library is the primary API.\n", "\n", "## Loading the Forest Fire Dataset\n", "\n", "Once again we'll use the [Forest Fire dataset](https://archive.ics.uci.edu/ml/datasets/Forest+Fires) from UCI, which contains data about the area burned by wildfires in the Northeast region of Portugal in relation to numerous other factors.\n", "\n", "Because we are using the DataFrame-based Spark MLlib, we'll load it as a Spark SQL DataFrame."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Create a SparkSession to connect to Spark local cluster\n", "from pyspark.sql import SparkSession\n", "spark = SparkSession.builder.master('local').getOrCreate()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "|  X|  Y|month|day|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|\n", "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "|  7|  5|  mar|fri|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|\n", "|  7|  4|  oct|tue|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|\n", "|  7|  4|  oct|sat|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|\n", "|  8|  6|  mar|fri|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|\n", "|  8|  6|  mar|sun|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|\n", "+---+---+-----+---+----+----+-----+---+----+---+----+----+----+\n", "only showing top 5 rows\n", "\n"]}], "source": ["# Read in local CSV file to a Spark SQL DataFrame\n", "fire_df = spark.read.csv('forestfires.csv', header='true', inferSchema='true')\n", "fire_df.show(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Machine Learning with Spark\n", "\n", "PySpark states that they've used scikit-learn as an inspiration for their implementation of a machine learning library. As a result, many of the methods and functionalities look similar, but there are some crucial distinctions. There are three main concepts found within the ML library:\n", "\n", "`Transformer`: An algorithm that transforms one PySpark DataFrame into another DataFrame. Just like in scikit-learn, a Transformer is a class that implements the `transform()` method. Transformers include both feature transformers and `Model`s.\n", "\n", "`Estimator`: An algorithm that can be fit onto a PySpark DataFrame that can then be used as a Transformer. Just like in scikit-learn, an Estimator is a class that implements the `fit()` method.\n", "\n", "`Pipeline`: A pipeline very similar to a scikit-learn pipeline that chains together different actions. Typically Transformers and Estimators are components of a Pipeline.\n", "\n", "The reasoning behind this separation of the fitting and transforming step is because Spark is lazily evaluated, so the 'fitting' of a model does not actually take place until the Transformation action is called. Let's examine what this actually looks like by performing a regression on the Forest Fire dataset. To start off with, we'll import the necessary machine learning classes for our tasks."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["from pyspark.ml.regression import RandomForestRegressor\n", "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### ML Preprocessing with Spark\n", "\n", "#### Encoding Categorical Data"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- X: integer (nullable = true)\n", " |-- Y: integer (nullable = true)\n", " |-- month: string (nullable = true)\n", " |-- day: string (nullable = true)\n", " |-- FFMC: double (nullable = true)\n", " |-- DMC: double (nullable = true)\n", " |-- DC: double (nullable = true)\n", " |-- ISI: double (nullable = true)\n", " |-- temp: double (nullable = true)\n", " |-- RH: integer (nullable = true)\n", " |-- wind: double (nullable = true)\n", " |-- rain: double (nullable = true)\n", " |-- area: double (nullable = true)\n", "\n"]}], "source": ["fire_df.printSchema()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looking at our data, one can see that all the categories are numeric except for `month` and `day`. We saw some correlation between the month and area burned in a fire during our previous EDA process, so we will include that in our model. The day of the week, however, is highly unlikely to have any effect on fire, so we will drop it from the DataFrame."]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+----+----+-----+---+----+---+----+----+----+\n", "|  X|  Y|month|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+\n", "|  7|  5|  mar|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|\n", "|  7|  4|  oct|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|\n", "|  7|  4|  oct|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|\n", "|  8|  6|  mar|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|\n", "|  8|  6|  mar|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+\n", "only showing top 5 rows\n", "\n"]}], "source": ["df_without_day = fire_df.drop('day')\n", "df_without_day.show(5)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- X: integer (nullable = true)\n", " |-- Y: integer (nullable = true)\n", " |-- month: string (nullable = true)\n", " |-- FFMC: double (nullable = true)\n", " |-- DMC: double (nullable = true)\n", " |-- DC: double (nullable = true)\n", " |-- ISI: double (nullable = true)\n", " |-- temp: double (nullable = true)\n", " |-- RH: integer (nullable = true)\n", " |-- wind: double (nullable = true)\n", " |-- rain: double (nullable = true)\n", " |-- area: double (nullable = true)\n", "\n"]}], "source": ["df_without_day.printSchema()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order for us to run our model, we need to turn the months variable into a dummy variable. In MLlib this is a 2-step process that first requires turning the categorical variable into a numerical index (`StringIndexer`). Only after the variable is an integer can PySpark create dummy variable columns related to each category (`OneHotEncoder`).\n", "\n", "Your key parameters when using these transformers are:\n", "\n", "* `inputCol` (the column you want to change), and\n", "* `outputCol` (where you will store the changed column)\n", "\n", "Here it is in action: "]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+\n", "|  X|  Y|month|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|month_num|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+\n", "|  7|  5|  mar|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|      2.0|\n", "|  7|  4|  oct|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|      6.0|\n", "|  7|  4|  oct|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|      6.0|\n", "|  8|  6|  mar|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|      2.0|\n", "|  8|  6|  mar|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|      2.0|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+\n", "only showing top 5 rows\n", "\n"]}], "source": ["# Create a StringIndexer (Estimator) that takes month as the input column\n", "# and outputs month_num\n", "si_untrained = StringIndexer(inputCol='month', outputCol='month_num', handleInvalid='keep')\n", "\n", "# Create a trained Model (Transformer) by fitting the StringIndexer on the data\n", "si_trained = si_untrained.fit(df_without_day)\n", "\n", "# Create a new DataFrame using the trained Model (Transformer) and data\n", "df_month_num = si_trained.transform(df_without_day)\n", "\n", "df_month_num.show(5)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---------+\n", "|month_num|\n", "+---------+\n", "|      0.0|\n", "|      1.0|\n", "|      2.0|\n", "|      3.0|\n", "|      4.0|\n", "|      5.0|\n", "|      6.0|\n", "|      7.0|\n", "|      8.0|\n", "|      9.0|\n", "|     10.0|\n", "|     11.0|\n", "+---------+\n", "\n"]}], "source": ["# Double-checking that we have 12 different numbers as expected\n", "df_month_num.select('month_num').distinct().orderBy('month_num').show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, we have created a new column called `month_num` that represents the month by a number.\n", "\n", "Note the small, but critical distinction between `sklearn`'s implementation of a transformer and PySpark's implementation. `sklearn` is more object oriented and Spark is more functional oriented.\n", "\n", "Specifically, calling `fit()` on a Spark transformer does not cause that transformer to become trained, but rather returns a trained transformer."]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["pyspark.ml.feature.StringIndexer"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["# this is an Estimator, which has a fit() method\n", "type(si_untrained)"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["pyspark.ml.feature.StringIndexerModel"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["# this is a Model (Transformer), the result of the fit() method\n", "type(si_trained)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Only the trained Model has fitted attributes. In this case, the `StringIndexerModel` has a `labels` attribute, which explain the relationship between `month_num` and `month`:"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": ["['aug',\n", " 'sep',\n", " 'mar',\n", " 'jul',\n", " 'feb',\n", " 'jun',\n", " 'oct',\n", " 'apr',\n", " 'dec',\n", " 'jan',\n", " 'may',\n", " 'nov']"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["si_trained.labels"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'AttributeError'>\n", "'StringIndexer' object has no attribute 'labels'\n"]}], "source": ["try:\n", "    si_untrained.labels\n", "except Exception as e:\n", "    print(type(e))\n", "    print(e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we have `month_num`, we can use Spark's version of `OneHotEncoder()`. This time rather than creating an intermediate variable for the model, we'll chain together the `fit()` and `tranform()` method calls."]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+\n", "|  X|  Y|month|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|month_num|     month_vec|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+\n", "|  7|  5|  mar|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|      2.0|(12,[2],[1.0])|\n", "|  7|  4|  oct|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|      6.0|(12,[6],[1.0])|\n", "|  7|  4|  oct|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|      6.0|(12,[6],[1.0])|\n", "|  8|  6|  mar|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|      2.0|(12,[2],[1.0])|\n", "|  8|  6|  mar|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|      2.0|(12,[2],[1.0])|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+\n", "only showing top 5 rows\n", "\n"]}], "source": ["# Instantiating, fitting, and transforming month_num with OneHotEncoder\n", "ohe = OneHotEncoder(inputCols=['month_num'], outputCols=['month_vec'], dropLast=True)\n", "df_month_vec = ohe.fit(df_month_num).transform(df_month_num)\n", "df_month_vec.show(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, we now have a one-hot encoded sparse vector in the `month_vec` column! Because Spark is optimized for big data, sparse vectors are used rather than entirely new columns for dummy variables because it is more space efficient. You can see in this first row of the DataFrame:\n", "\n", "```\n", "(11,[2],[1.0])\n", "```\n", "\n", "This indicates that:\n", "\n", "* We have a sparse vector of size `11` (because there are 12 unique month values, and we set `dropLast=True` when we instantiated the `OneHotEncoder`)\n", "* This particular data point is the `2`-th index of our month labels (`'mar'`, based off the labels in the `StringIndexerModel`)\n", "\n", "#### One Sparse Vector\n", "\n", "The final requirement for all machine learning models in PySpark is to put all of the features of your model into one sparse vector. This is once again for efficiency's sake. Here, we are doing that with the `VectorAssembler()` estimator."]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/plain": ["['X',\n", " 'Y',\n", " 'FFMC',\n", " 'DMC',\n", " 'DC',\n", " 'ISI',\n", " 'temp',\n", " 'RH',\n", " 'wind',\n", " 'rain',\n", " 'month_vec']"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["# Set name of target\n", "target = \"area\"\n", "\n", "# Get list of features, excluding area (the target) as well\n", "# as month and month_num (since we are using month_vec)\n", "features = df_month_vec.drop(target, \"month\", \"month_num\").columns\n", "features"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+\n", "|  X|  Y|month|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|month_num|     month_vec|            features|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+\n", "|  7|  5|  mar|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|\n", "|  7|  4|  oct|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...|\n", "|  7|  4|  oct|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...|\n", "|  8|  6|  mar|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|\n", "|  8|  6|  mar|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+\n", "only showing top 5 rows\n", "\n"]}], "source": ["# A VectorAssembler is a \"pure\" Transformer, not a Model that\n", "# requires fitting. Use it to transform features.\n", "va = VectorAssembler(inputCols=features, outputCol=\"features\")\n", "df_vec = va.transform(df_month_vec)\n", "\n", "df_vec.show(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is definitely a bit weird to look at compared to data prepared in pandas for a scikit-learn model. What you need to know is that all of the feature data that the model will actually use is encoded in the `features` column."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Fitting an ML Model with Spark\n", "\n", "Great! We now have our data in a format that seems acceptable for the last step. It's time for us to actually fit our model to data! Let's fit a Random Forest Regression model to our data. Although there are still a bunch of other features in the DataFrame, it doesn't matter for the machine learning model API. All that needs to be specified are the names of the features column and the label (i.e. target) column. "]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# Instantiating and fitting the model\n", "rf_model = RandomForestRegressor(featuresCol='features', \n", "                                 labelCol=target,\n", "                                 predictionCol='prediction'\n", "                                ).fit(df_vec)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"data": {"text/plain": ["SparseVector(22, {0: 0.1139, 1: 0.0508, 2: 0.2052, 3: 0.1079, 4: 0.1395, 5: 0.0359, 6: 0.0752, 7: 0.1318, 8: 0.1093, 9: 0.002, 10: 0.0018, 11: 0.0142, 12: 0.0, 13: 0.0107, 14: 0.0002, 16: 0.001, 17: 0.0001, 18: 0.0001, 20: 0.0003})"]}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": ["# Inspecting fitted model attributes\n", "rf_model.featureImportances"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "|  X|  Y|month|FFMC| DMC|   DC|ISI|temp| RH|wind|rain|area|month_num|     month_vec|            features|        prediction|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "|  7|  5|  mar|86.2|26.2| 94.3|5.1| 8.2| 51| 6.7| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...| 5.898088759947248|\n", "|  7|  4|  oct|90.6|35.4|669.1|6.7|18.0| 33| 0.9| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...| 5.005294139647212|\n", "|  7|  4|  oct|90.6|43.7|686.9|6.7|14.6| 33| 1.3| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...|5.9012003896472125|\n", "|  8|  6|  mar|91.7|33.3| 77.5|9.0| 8.3| 97| 4.0| 0.2| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|7.7049237280786285|\n", "|  8|  6|  mar|89.3|51.3|102.2|9.6|11.4| 99| 1.8| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|4.7609687837235715|\n", "+---+---+-----+----+----+-----+---+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "only showing top 5 rows\n", "\n"]}], "source": ["# Transform the DataFrame using the fitted model\n", "df_final = rf_model.transform(df_vec)\n", "df_final.show(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, this looks very different from our usual scikit-learn approach. Instead of generating a prediction as a separate vector, it was added as another column in the overall DataFrame.\n", "\n", "### Model Evaluation with Spark\n", "\n", "If we want to look at just the target vs. the prediction, we can select just those columns:"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|area|        prediction|\n", "+----+------------------+\n", "| 0.0| 5.898088759947248|\n", "| 0.0| 5.005294139647212|\n", "| 0.0|5.9012003896472125|\n", "| 0.0|7.7049237280786285|\n", "| 0.0|4.7609687837235715|\n", "| 0.0| 7.910257549593487|\n", "| 0.0| 7.146172361255255|\n", "| 0.0|11.923624873519476|\n", "| 0.0| 7.499414846964632|\n", "| 0.0| 6.922794908436825|\n", "+----+------------------+\n", "only showing top 10 rows\n", "\n"]}], "source": ["df_final.select(target, 'prediction').show(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok, this snapshot is showing the model consistently predicting too high of an area.\n", "\n", "What if we look at the subset where the area was greater than zero?"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|area|        prediction|\n", "+----+------------------+\n", "|0.36|3.7585944317835063|\n", "|0.43|15.022976888053885|\n", "|0.47|11.840964939026374|\n", "|0.55| 7.938726300414726|\n", "|0.61| 3.992752285380399|\n", "|0.71|13.697652912644354|\n", "|0.77| 4.596214663344088|\n", "| 0.9|11.419223850936309|\n", "|0.95|4.4435624079080895|\n", "|0.96| 18.59024169502403|\n", "+----+------------------+\n", "only showing top 10 rows\n", "\n"]}], "source": ["df_final[df_final[target] > 0].select(target, 'prediction').show(10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Still not so great. Let's use some metrics to evaluate formally.\n", "\n", "PySpark has a class `RegressionEvaluator` that is designed for this purpose:"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["from pyspark.ml.evaluation import RegressionEvaluator\n", "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol=target)"]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.582721694127662"]}, "execution_count": 22, "metadata": {}, "output_type": "execute_result"}], "source": ["# Evaluating r^2\n", "evaluator.evaluate(df_final, {evaluator.metricName: 'r2'})"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"data": {"text/plain": ["13.464552164786383"]}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": ["# Evaluating mean absolute error\n", "evaluator.evaluate(df_final, {evaluator.metricName: 'mae'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In other words, our model is able to explain about 80% of the variance in the data, and on average its area prediction is off by about 13 hectares."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Putting It All in a Pipeline\n", "\n", "We just performed a whole lot of transformations to our data. Let's take a look at all the estimators we used to create this model:\n", "\n", "* `StringIndexer()` \n", "* `OneHotEncoder()` \n", "* `VectorAssembler()` \n", "* `RandomForestRegressor()` \n", "\n", "Once we've fit our model in the Pipeline, we're then going to want to evaluate it to determine how well it performs. We can do this with:\n", "\n", "* `RegressionEvaluator()` \n", "\n", "We can streamline all of these transformations to make it much more efficient by chaining them together in a pipeline. The Pipeline object expects a list of the estimators to be set to the parameter `stages`."]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'month_vec']\n", "area\n"]}], "source": ["# Recall that we have already established features and target variables\n", "print(features)\n", "print(target)"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["# Importing relevant class\n", "from pyspark.ml import Pipeline\n", "\n", "# Creating a variable for this one because we will need it later\n", "# Using the features and target columns, create a column of predictions\n", "random_forest = RandomForestRegressor(featuresCol='features', labelCol=target, predictionCol='prediction')\n", "\n", "# Instantiating the pipeline with all the estimators\n", "pipeline = Pipeline(stages=[\n", "    # Convert string data in 'month' column to numeric data in 'month_num' column\n", "    StringIndexer(inputCol='month', outputCol='month_num', handleInvalid='keep'),\n", "    # Convert 'month_num' numbers to sparse vector in 'month_vec' column\n", "    OneHotEncoder(inputCols=['month_num'], outputCols=['month_vec'], dropLast=True),\n", "    # Convert all of the relevant features to sparse vector in 'features' column\n", "    VectorAssembler(inputCols=features, outputCol='features'),\n", "    random_forest\n", "])"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+---+---+-----+---+----+-----+-----+----+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "|  X|  Y|month|day|FFMC|  DMC|   DC| ISI|temp| RH|wind|rain|area|month_num|     month_vec|            features|        prediction|\n", "+---+---+-----+---+----+-----+-----+----+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "|  7|  5|  mar|fri|86.2| 26.2| 94.3| 5.1| 8.2| 51| 6.7| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...| 5.898088759947248|\n", "|  7|  4|  oct|tue|90.6| 35.4|669.1| 6.7|18.0| 33| 0.9| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...| 5.005294139647212|\n", "|  7|  4|  oct|sat|90.6| 43.7|686.9| 6.7|14.6| 33| 1.3| 0.0| 0.0|      6.0|(12,[6],[1.0])|(22,[0,1,2,3,4,5,...|5.9012003896472125|\n", "|  8|  6|  mar|fri|91.7| 33.3| 77.5| 9.0| 8.3| 97| 4.0| 0.2| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|7.7049237280786285|\n", "|  8|  6|  mar|sun|89.3| 51.3|102.2| 9.6|11.4| 99| 1.8| 0.0| 0.0|      2.0|(12,[2],[1.0])|(22,[0,1,2,3,4,5,...|4.7609687837235715|\n", "|  8|  6|  aug|sun|92.3| 85.3|488.0|14.7|22.2| 29| 5.4| 0.0| 0.0|      0.0|(12,[0],[1.0])|(22,[0,1,2,3,4,5,...| 7.910257549593487|\n", "|  8|  6|  aug|mon|92.3| 88.9|495.6| 8.5|24.1| 27| 3.1| 0.0| 0.0|      0.0|(12,[0],[1.0])|(22,[0,1,2,3,4,5,...| 7.146172361255255|\n", "|  8|  6|  aug|mon|91.5|145.4|608.2|10.7| 8.0| 86| 2.2| 0.0| 0.0|      0.0|(12,[0],[1.0])|(22,[0,1,2,3,4,5,...|11.923624873519476|\n", "|  8|  6|  sep|tue|91.0|129.5|692.6| 7.0|13.1| 63| 5.4| 0.0| 0.0|      1.0|(12,[1],[1.0])|(22,[0,1,2,3,4,5,...| 7.499414846964632|\n", "|  7|  5|  sep|sat|92.5| 88.0|698.6| 7.1|22.8| 40| 4.0| 0.0| 0.0|      1.0|(12,[1],[1.0])|(22,[0,1,2,3,4,5,...| 6.922794908436825|\n", "+---+---+-----+---+----+-----+-----+----+----+---+----+----+----+---------+--------------+--------------------+------------------+\n", "only showing top 10 rows\n", "\n"]}], "source": ["# Now we can perform all of the above preprocessing steps!\n", "df_final_pipeline = pipeline.fit(fire_df).transform(fire_df)\n", "df_final_pipeline.show(10)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.582721694127662"]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": ["# Evaluating r^2\n", "evaluator.evaluate(df_final_pipeline, {evaluator.metricName: 'r2'})"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"data": {"text/plain": ["13.464552164786383"]}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": ["# Evaluating mean absolute error\n", "evaluator.evaluate(df_final_pipeline, {evaluator.metricName: 'mae'})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And we're getting the same metrics as before!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Model Tuning\n", "\n", "You might have missed a critical step in the random forest regression above; we did not cross validate or perform a train/test split! This means that we don't have a good idea of how our model would perform on unseen data, or have a good setup for model tuning.\n", "\n", "Fortunately pipelines in MLlib make this process fairly straightforward. Some components to be aware of are:\n", "\n", "* `ParamGridBuilder` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html)): This is similar to `GridSearchCV` in scikit-learn.\n", "  * It allows you to specify and test out various combinations of parameters.\n", "  * Also like `GridSearchCV`, it is easy to write code that will take a **very** long time to execute, so be cautious with the number of models you try to run at once!\n", "  * Unlike `GridSearchCV`, it isn't necessarily used with a cross-validation strategy. You can choose between using cross-validation or a train-validation split depending on the context.\n", "* `CrossValidator` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html)): This is similar to `cross_val_score` in scikit-learn, although it's a class rather than a function.\n", "  * It allows you to perform k-fold cross-validation, to get model metrics across several different splits of data.\n", "* `TrainValidationSplit` ([documentation here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.TrainValidationSplit.html)): There is no direct parallel to this in scikit-learn, although it contains elements of `train_test_split`.\n", "  * It allows you to perform a train-validation split, and to evaluate a model based on a validation set that the model was not trained on.\n", "\n", "#### Comparing `CrossValidator` and `TrainValidationSplit`\n", "  \n", "`CrossValidator` is more computationally expensive but will likely give you a more realistic view of how your model will perform on unseen data. `TrainValidationSplit` is faster but potentially less realistic, because it is relying on a single split of the data. For this lesson we'll only use `CrossValidator`, and we expect the grid search to take up to a couple of minutes. If your computer is running very slowly, you might consider changing it to a `TrainValidationSplit`, although the numbers will come out a bit different.\n", "\n", "#### Building Our Param Grid\n", "\n", "Let's go ahead and import those classes, then build our param grid!\n", "\n", "Note that we are specifying hyperparameters for the `RandomForestRegressor`, documentation for which can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html)."]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["# Model tuning imports\n", "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["# Creating parameter grid \n", "\n", "# (This style of multi-line code with backslashes imitates the\n", "# style of Scala, the original Spark languge)\n", "\n", "params = ParamGridBuilder()\\\n", "          .addGrid(random_forest.maxDepth, [5, 10, 15])\\\n", "          .addGrid(random_forest.numTrees, [20, 50, 100])\\\n", "          .build()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's take a look at the params variable we just built."]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["total combinations of parameters:  9\n"]}, {"data": {"text/plain": ["{Param(parent='RandomForestRegressor_bc564c30865e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n", " Param(parent='RandomForestRegressor_bc564c30865e', name='numTrees', doc='Number of trees to train (>= 1).'): 20}"]}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": ["print('total combinations of parameters: ', len(params))\n", "\n", "params[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Cross-Validation\n", "\n", "Now it's time to combine all the steps we've created to work in a single line of code with the `CrossValidator()` estimator."]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["# Instantiating the evaluator by which we will measure our model's performance\n", "reg_evaluator = RegressionEvaluator(\n", "    # 'prediction' is where our model is adding the prediction to the df\n", "    predictionCol='prediction',\n", "    # The evaluator is comparing the prediction to this target\n", "    labelCol=target,\n", "    # The evaluator is evaluation the prediction based on this metric\n", "    metricName = 'mae'\n", ")\n", "\n", "# Instantiating cross validator estimator\n", "cv = CrossValidator(\n", "    # Will call fit and transform using this\n", "    estimator=pipeline,\n", "    # Will iterate over these hyperparameter options\n", "    estimatorParamMaps=params,\n", "    # Will evaluate based on this evaluator\n", "    evaluator=reg_evaluator,\n", "    # Will use 4 threads when running parallel algorithms\n", "    parallelism=4\n", ")"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# Fitting cross validator\n", "cross_validated_model = cv.fit(fire_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Model Evaluation with Cross-Validation\n", "\n", "Now, let's see how well the model performed! Let's take a look at the average performance for each one of our 9 models."]}, {"cell_type": "code", "execution_count": 34, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/plain": ["[22.790540868526982,\n", " 20.69425857338582,\n", " 21.054343931678073,\n", " 23.376957675022418,\n", " 21.56271738942795,\n", " 22.00020398191387,\n", " 23.556214611760687,\n", " 21.67247430231654,\n", " 22.088303223954767]"]}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": ["cross_validated_model.avgMetrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It looks like the optimal performance is an MAE around 20. Note that this is worse than our original model, but that's because our original model had substantial data leakage. We didn't do a train-test split!\n", "\n", "Now, let's take a look at the optimal parameters of our best performing model. The `cross_validated_model` variable is now saved as the best performing model from the grid search just performed. Let's look to see how well the predictions performed."]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+----+------------------+\n", "|area|        prediction|\n", "+----+------------------+\n", "| 0.0|  5.46338663408609|\n", "| 0.0| 5.060755772318182|\n", "| 0.0| 5.737809143630919|\n", "| 0.0| 4.748977483241895|\n", "| 0.0| 5.332632130720389|\n", "| 0.0|11.612369608132388|\n", "| 0.0|19.185312316761088|\n", "| 0.0| 6.216435834110016|\n", "| 0.0| 7.379463598849831|\n", "| 0.0|28.065495368422503|\n", "| 0.0|27.812209937124873|\n", "| 0.0| 5.794094576555795|\n", "| 0.0| 6.037209276188649|\n", "| 0.0| 9.290245863672006|\n", "| 0.0| 47.24540019586294|\n", "| 0.0| 6.795623362962512|\n", "| 0.0| 7.873476144591231|\n", "| 0.0|  6.73460316419175|\n", "| 0.0| 4.760671946056203|\n", "| 0.0| 6.048905897961223|\n", "| 0.0|10.584247992057055|\n", "| 0.0| 4.546657880529267|\n", "| 0.0| 5.467282366824678|\n", "| 0.0| 10.27961856817314|\n", "| 0.0| 6.272015015582582|\n", "| 0.0| 7.083985362121085|\n", "| 0.0| 6.203432688364173|\n", "| 0.0|11.375783017521048|\n", "| 0.0|12.000010639657148|\n", "| 0.0|11.045536313365655|\n", "| 0.0| 6.850173373084945|\n", "| 0.0| 6.436397524928238|\n", "| 0.0|15.786882778282767|\n", "| 0.0| 4.738732270196554|\n", "| 0.0| 4.645643437990029|\n", "| 0.0| 6.758133775205431|\n", "| 0.0|26.383972715575528|\n", "| 0.0| 6.105761375078693|\n", "| 0.0| 16.61255671674599|\n", "| 0.0| 4.690559598014674|\n", "| 0.0| 9.708479146506564|\n", "| 0.0|4.8104096616387775|\n", "| 0.0| 5.074192503399149|\n", "| 0.0| 5.553098795684662|\n", "| 0.0|  5.66870724297994|\n", "| 0.0| 46.56302959069151|\n", "| 0.0| 6.990206865456335|\n", "| 0.0| 6.872222467122986|\n", "| 0.0| 4.223508890271444|\n", "| 0.0| 5.970183185158617|\n", "| 0.0|10.969848455587883|\n", "| 0.0|4.8213335947089515|\n", "| 0.0| 4.832069050732673|\n", "| 0.0| 4.832069050732673|\n", "| 0.0| 5.032004491593833|\n", "| 0.0|12.625781216658796|\n", "| 0.0|  5.86006558595974|\n", "| 0.0| 4.232461657746175|\n", "| 0.0| 4.700484051723977|\n", "| 0.0|7.5203075497526655|\n", "| 0.0| 4.349877014709304|\n", "| 0.0| 4.932143265278998|\n", "| 0.0| 6.218594931219074|\n", "| 0.0| 3.770991003852627|\n", "| 0.0|3.5755725049353493|\n", "| 0.0| 5.552974784039699|\n", "| 0.0| 8.431787735706902|\n", "| 0.0| 8.102125645595374|\n", "| 0.0| 8.155875645595376|\n", "| 0.0| 7.362754185264508|\n", "| 0.0|4.3992305382614205|\n", "| 0.0|5.1979957782289405|\n", "| 0.0|3.7404778984107416|\n", "| 0.0| 6.068568566409702|\n", "| 0.0|10.084174124725102|\n", "| 0.0| 5.629655652994955|\n", "| 0.0| 6.069193623171193|\n", "| 0.0| 5.812605822061212|\n", "| 0.0| 6.357484240943912|\n", "| 0.0| 7.732573151435336|\n", "| 0.0|  8.88097924640188|\n", "| 0.0|3.6173302019865634|\n", "| 0.0|11.014943899365527|\n", "| 0.0|4.1900626402606465|\n", "| 0.0| 9.736500524064574|\n", "| 0.0|16.975234821441457|\n", "| 0.0|10.827874796601314|\n", "| 0.0| 18.89312940809614|\n", "| 0.0|18.639289851456066|\n", "| 0.0| 4.479286611101133|\n", "| 0.0| 5.309372844108085|\n", "| 0.0|23.413515124032607|\n", "| 0.0|15.258117940358936|\n", "| 0.0|11.249815128940634|\n", "| 0.0|19.327886062026554|\n", "| 0.0| 4.850396225963741|\n", "| 0.0| 4.387368083936174|\n", "| 0.0|3.8455484381241503|\n", "| 0.0|4.6959124089551185|\n", "| 0.0| 5.428679997952353|\n", "| 0.0| 5.428679997952353|\n", "| 0.0| 5.789739085587703|\n", "| 0.0| 4.963103861294058|\n", "| 0.0|26.499083032394857|\n", "| 0.0| 4.723418229931103|\n", "| 0.0| 4.937193856356565|\n", "| 0.0|4.5945951301013315|\n", "| 0.0| 4.715511062561639|\n", "| 0.0|  27.0094161912567|\n", "| 0.0|5.6320755706073165|\n", "| 0.0| 5.344408867643683|\n", "| 0.0|3.8947555966262697|\n", "| 0.0| 4.976256670669868|\n", "| 0.0| 4.330024079708588|\n", "| 0.0| 4.271870775006883|\n", "| 0.0| 4.225068323316062|\n", "| 0.0| 4.445092457102481|\n", "| 0.0| 5.003636721310558|\n", "| 0.0| 4.560152033136266|\n", "| 0.0| 4.781404358781996|\n", "| 0.0| 4.271186695099789|\n", "| 0.0|4.7473314387468415|\n", "| 0.0| 9.587345744738851|\n", "| 0.0| 6.803054081646768|\n", "| 0.0|  6.32744110190101|\n", "| 0.0| 4.061846155903323|\n", "| 0.0| 4.231800662416398|\n", "| 0.0| 9.764829829756211|\n", "| 0.0| 4.147417579978568|\n", "| 0.0| 3.452521718952903|\n", "| 0.0|  4.41170204936271|\n", "| 0.0| 4.223791264881175|\n", "| 0.0| 4.464015513054437|\n", "| 0.0| 5.432857057609448|\n", "| 0.0|3.7639627473391886|\n", "| 0.0|  5.54567247304219|\n", "| 0.0| 9.214554548963669|\n", "| 0.0| 6.848283564876156|\n", "|0.36|  17.3784647029196|\n", "|0.43| 16.43628746559906|\n", "|0.47|   8.7872870422044|\n", "|0.55| 5.747220413964902|\n", "|0.61|16.993610378051052|\n", "|0.71| 8.480749839393368|\n", "|0.77| 4.815672912356325|\n", "| 0.9| 7.308354476968933|\n", "|0.95| 5.196321908174747|\n", "|0.96|11.529026673684323|\n", "|1.07| 7.273420127410195|\n", "|1.12| 21.78304149084823|\n", "+----+------------------+\n", "only showing top 150 rows\n", "\n"]}], "source": ["predictions = cross_validated_model.transform(fire_df)\n", "predictions.select('area', 'prediction').show(150)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, this dataset has a large number of areas of \"0.0\" burned. Perhaps it would be better to investigate this problem as a classification task.\n", "\n", "Now let's go ahead and take a look at the feature importances of our random forest model. In order to do this, we need to unroll our pipeline to access the random forest model. Let's start by first checking out the `.bestModel` attribute of our `cross_validated_model`. "]}, {"cell_type": "code", "execution_count": 36, "metadata": {"slideshow": {"slide_type": "slide"}}, "outputs": [{"data": {"text/plain": ["pyspark.ml.pipeline.PipelineModel"]}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}], "source": ["type(cross_validated_model.bestModel)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MLlib is treating the entire pipeline as the best performing model, so we need to go deeper into the pipeline to access the random forest model within it. Previously, we put the random forest model as the final \"stage\" in the stages variable list. Let's look at the `.stages` attribute of the `.bestModel`."]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"data": {"text/plain": ["[StringIndexerModel: uid=StringIndexer_c22019728c37, handleInvalid=keep,\n", " OneHotEncoderModel: uid=OneHotEncoder_21bc973c69a4, dropLast=true, handleInvalid=error, numInputCols=1, numOutputCols=1,\n", " VectorAssembler_2dc86ea63350,\n", " RandomForestRegressionModel: uid=RandomForestRegressor_bc564c30865e, numTrees=50, numFeatures=22]"]}, "execution_count": 37, "metadata": {}, "output_type": "execute_result"}], "source": ["cross_validated_model.bestModel.stages"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Perfect! There's the RandomForestRegressionModel, represented by the last item in the stages list. Now, we should be able to access all the attributes of the random forest regressor."]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["optimal_rf_model = cross_validated_model.bestModel.stages[3]"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"data": {"text/plain": ["SparseVector(22, {0: 0.0812, 1: 0.0675, 2: 0.1677, 3: 0.1252, 4: 0.1171, 5: 0.0689, 6: 0.1147, 7: 0.1173, 8: 0.0966, 9: 0.0, 10: 0.0092, 11: 0.011, 12: 0.0, 13: 0.0169, 14: 0.0004, 15: 0.0056, 16: 0.0001, 17: 0.0003, 18: 0.0002, 20: 0.0})"]}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": ["optimal_rf_model.featureImportances"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"data": {"text/plain": ["50"]}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": ["optimal_rf_model.getNumTrees"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "In this lesson, you learned about PySpark's machine learning models and pipelines. With the use of a pipeline, you can train a huge number of models simultaneously, saving you a substantial amount of time and effort. Up next, you will have a chance to build a PySpark machine learning pipeline of your own with a classification problem!"]}], "metadata": {"kernelspec": {"display_name": "Python (python3)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.5"}}, "nbformat": 4, "nbformat_minor": 2}