{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47614a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #entry point for pyspark\n",
    "\n",
    "#instantiate spark instance\n",
    "spark = SparkSession.builder.appName('Random Forest Iris').master(\"local[*]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bf25e",
   "metadata": {},
   "source": [
    "After version 3.0, `SparkSession` is the main entry point for Spark. `SparkSession.builder` creates a spark session. Any thing can go into the `appName()` to specify which jobs you are running currently. Once the spark session is instantiated, you can access the Spark UI at `localhost:4040` to view jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/IRIS.csv', header=True, inferSchema=True)\n",
    "df.printSchema() #to see the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show() # or df.show(Truncate=false) if you'd like to see all the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92324daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do more analysis if necessary on the data, and feel free to use pandas library\n",
    "# import pandas as pd\n",
    "# pd.DataFrame(df.take(10), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1f26b",
   "metadata": {},
   "source": [
    "Once the exploratory data analysis is done, we can start feature transforming to prepare for feataure engineering. Feature transforming means scaling, modifying features to be used for train/test validation, converting, etc. For this purpose, we can use `VectorAssembler` in PySpark.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "numeric_cols = [] #insert numeric cols\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df) #just use the same dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97297ee9",
   "metadata": {},
   "source": [
    "This should have created another column in your dataframe called `features` as we have denoted in `outputCol`. Now, we can use the `StringIndexer` to encode the string column of species to a label indicies. By default, the labels are assigned according to the frequencies (for imbalanced dataset). The most frequent species would get an index of 0. For balanced dataset, whichever string appears first will get 0, then so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd89a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labeler = StringIndexer(inputCol=\"features\", outputCol=\"encoded\")\n",
    "df = labeler.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866286e1",
   "metadata": {},
   "source": [
    "You should be able to see the new column named `encoded` with new values populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df130f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try doing this if you've already imported pandas\n",
    "# pd.DataFrame(df.take(10), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da29d5",
   "metadata": {},
   "source": [
    "Now we have transformed the data as we needed, we can now split the data into train/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0debe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42) #feel free to change the numbers in the random split or seed\n",
    "print(f\"Train dataset count: {str(train.count())}\")\n",
    "print(f\"Test dataset count: {str(test.count())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc84832",
   "metadata": {},
   "source": [
    "Let's instantiate the `RandomForestClassifier` and run the model. At this point, feel free to pull up the Spark UI from `localhost:4040` and examine the executors tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7efe0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"encoded\")\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the columns names here are different, do a `printSchema` on top of predictions to see the correct column names\n",
    "predictions.select('sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'encoded', 'rawPrediction', 'prediction', 'probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7d1d0",
   "metadata": {},
   "source": [
    "`featuresCol` is the list of features of the dataframe, which means if you have more features you'd like to include, you could put in a list. We create a model by fitting the training dataset, then predict on using the test dataset. `model.transform(test)` will create new columns, like `rawPrediction`, `prediction`, and `probability`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba54d6f",
   "metadata": {},
   "source": [
    "Now we've built a model, let's evaluate the model by using `MulticlassClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='encoded', predictionCol='prediction')\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(f\"Accuracy: {accuracy}%\")\n",
    "test_error = 1.0 - accuracy\n",
    "print(f\"Test Error = {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ca4de",
   "metadata": {},
   "source": [
    "Question: How did we perform on the model? What other metrics can we use to present if the classification models performed well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7dfedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
