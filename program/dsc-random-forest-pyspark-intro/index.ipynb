{"cells": [{"cell_type": "markdown", "id": "036a6ef0", "metadata": {}, "source": ["# Random Forest with Pyspark Introduction  "]}, {"cell_type": "markdown", "id": "01546685", "metadata": {}, "source": ["## Introduction\n", "\n", "In this lesson, you will walk through how to use PySpark for the classification of Iris flowers with a Random Forest Classifier. The dataset is located under the `data` folder."]}, {"cell_type": "markdown", "id": "35042fb4", "metadata": {}, "source": ["## Objectives  \n", "\n", "* Read a dataset into a PySpark DataFrame\n", "* Implement a random forest classifier with PySpark"]}, {"cell_type": "markdown", "id": "403c46dd", "metadata": {}, "source": ["> Before continuing, check the version of PySpark installed on the machine. It should be above 3.1.\n", "> \n", "> You will run this notebook in a `pyspark-env` environment following [these setup instructions without docker](https://github.com/learn-co-curriculum/dsc-spark-docker-installation)"]}, {"cell_type": "code", "execution_count": 1, "id": "47614a5a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["WARNING: An illegal reflective access operation has occurred\n", "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/pisel/opt/anaconda3/envs/spark-env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n", "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n", "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n", "WARNING: All illegal access operations will be denied in a future release\n", "23/09/07 11:44:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n", "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n", "Setting default log level to \"WARN\".\n", "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]}], "source": ["from pyspark.sql import SparkSession  # entry point for pyspark\n", "\n", "# instantiate spark instance\n", "spark = (\n", "    SparkSession.builder.appName(\"Random Forest Iris\").master(\"local[*]\").getOrCreate()\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "a28bf25e", "metadata": {}, "source": ["After version 3.0, `SparkSession` is the main entry point for Spark. `SparkSession.builder` creates a spark session. Any thing can go into the `appName()` to specify which jobs you are running currently. Once the spark session is instantiated, if you are running on your local machine, you can access the Spark UI at `localhost:4040` to view jobs."]}, {"cell_type": "code", "execution_count": 2, "id": "6b33c91c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["                                                                                \r"]}, {"name": "stdout", "output_type": "stream", "text": ["root\n", " |-- sepal_length: double (nullable = true)\n", " |-- sepal_width: double (nullable = true)\n", " |-- petal_length: double (nullable = true)\n", " |-- petal_width: double (nullable = true)\n", " |-- species: string (nullable = true)\n", "\n"]}], "source": ["df = spark.read.csv(\"./data/IRIS.csv\", header=True, inferSchema=True)\n", "df.printSchema()  # to see the schema"]}, {"cell_type": "code", "execution_count": 3, "id": "bd2d4f86", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+------------+-----------+------------+-----------+-----------+\n", "|sepal_length|sepal_width|petal_length|petal_width|    species|\n", "+------------+-----------+------------+-----------+-----------+\n", "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n", "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n", "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n", "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n", "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n", "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n", "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n", "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n", "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n", "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n", "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n", "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n", "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n", "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n", "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n", "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n", "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n", "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n", "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n", "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n", "+------------+-----------+------------+-----------+-----------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["df.show()  # or df.show(Truncate=false) if you'd like to see all the contents"]}, {"cell_type": "markdown", "id": "7291b8e0-270a-45dd-92d0-177241a3dd35", "metadata": {}, "source": ["Check to see what the type is for the DataFrame you have loaded."]}, {"cell_type": "code", "execution_count": 4, "id": "297df130-05f6-4bfb-a199-aadb3689df97", "metadata": {}, "outputs": [{"data": {"text/plain": ["pyspark.sql.dataframe.DataFrame"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["type(df)"]}, {"cell_type": "markdown", "id": "0d9f5aa3-cc86-4ad6-a114-5cb29469f24e", "metadata": {}, "source": ["Go ahead and run some exploratory data analysis on the dataset. You can easily turn the PySpark DataFrame into a Pandas DataFrame."]}, {"cell_type": "code", "execution_count": 5, "id": "92324daf", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>sepal_length</th>\n", "      <th>sepal_width</th>\n", "      <th>petal_length</th>\n", "      <th>petal_width</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>count</th>\n", "      <td>100.000000</td>\n", "      <td>100.000000</td>\n", "      <td>100.000000</td>\n", "      <td>100.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>mean</th>\n", "      <td>5.471000</td>\n", "      <td>3.094000</td>\n", "      <td>2.862000</td>\n", "      <td>0.785000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>std</th>\n", "      <td>0.641698</td>\n", "      <td>0.476057</td>\n", "      <td>1.448565</td>\n", "      <td>0.566288</td>\n", "    </tr>\n", "    <tr>\n", "      <th>min</th>\n", "      <td>4.300000</td>\n", "      <td>2.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.100000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>25%</th>\n", "      <td>5.000000</td>\n", "      <td>2.800000</td>\n", "      <td>1.500000</td>\n", "      <td>0.200000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>50%</th>\n", "      <td>5.400000</td>\n", "      <td>3.050000</td>\n", "      <td>2.450000</td>\n", "      <td>0.800000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>75%</th>\n", "      <td>5.900000</td>\n", "      <td>3.400000</td>\n", "      <td>4.325000</td>\n", "      <td>1.300000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>max</th>\n", "      <td>7.000000</td>\n", "      <td>4.400000</td>\n", "      <td>5.100000</td>\n", "      <td>1.800000</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["       sepal_length  sepal_width  petal_length  petal_width\n", "count    100.000000   100.000000    100.000000   100.000000\n", "mean       5.471000     3.094000      2.862000     0.785000\n", "std        0.641698     0.476057      1.448565     0.566288\n", "min        4.300000     2.000000      1.000000     0.100000\n", "25%        5.000000     2.800000      1.500000     0.200000\n", "50%        5.400000     3.050000      2.450000     0.800000\n", "75%        5.900000     3.400000      4.325000     1.300000\n", "max        7.000000     4.400000      5.100000     1.800000"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "\n", "pandas_df = pd.DataFrame(df.take(100), columns=df.columns)\n", "pandas_df.describe()"]}, {"cell_type": "code", "execution_count": 6, "id": "8d40db78-5ff1-47b5-bb8b-d0a75b741ebd", "metadata": {}, "outputs": [{"data": {"text/plain": ["sepal_length    float64\n", "sepal_width     float64\n", "petal_length    float64\n", "petal_width     float64\n", "species          object\n", "dtype: object"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["pandas_df.dtypes"]}, {"cell_type": "markdown", "id": "e5f1f26b", "metadata": {}, "source": ["Once the exploratory data analysis is done, you can start feature transforming to prepare for feataure engineering. Feature transforming means scaling, modifying features to be used for train/test validation, and converting. For this purpose, you will use the `VectorAssembler` in PySpark."]}, {"cell_type": "code", "execution_count": 7, "id": "8cfd6b6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+------------+-----------+------------+-----------+-----------+-----------------+\n", "|sepal_length|sepal_width|petal_length|petal_width|    species|         features|\n", "+------------+-----------+------------+-----------+-----------+-----------------+\n", "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n", "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n", "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n", "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n", "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n", "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|[5.4,3.9,1.7,0.4]|\n", "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|[4.6,3.4,1.4,0.3]|\n", "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|[5.0,3.4,1.5,0.2]|\n", "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|[4.4,2.9,1.4,0.2]|\n", "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|[4.9,3.1,1.5,0.1]|\n", "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|[5.4,3.7,1.5,0.2]|\n", "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|[4.8,3.4,1.6,0.2]|\n", "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|[4.8,3.0,1.4,0.1]|\n", "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|[4.3,3.0,1.1,0.1]|\n", "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|[5.8,4.0,1.2,0.2]|\n", "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|[5.7,4.4,1.5,0.4]|\n", "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|[5.4,3.9,1.3,0.4]|\n", "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|[5.1,3.5,1.4,0.3]|\n", "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|[5.7,3.8,1.7,0.3]|\n", "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|[5.1,3.8,1.5,0.3]|\n", "+------------+-----------+------------+-----------+-----------+-----------------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["from pyspark.ml.feature import VectorAssembler\n", "\n", "numeric_cols = [\n", "    \"sepal_length\",\n", "    \"sepal_width\",\n", "    \"petal_length\",\n", "    \"petal_width\",\n", "]  # insert numeric cols\n", "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n", "df = assembler.transform(df)  # just use the same dataframe\n", "df.show()"]}, {"cell_type": "markdown", "id": "97297ee9", "metadata": {}, "source": ["This should have created another column in your dataframe called `features` as you have denoted in `outputCol`. You can use the `StringIndexer` to encode the string column of species to a label index. By default, the labels are assigned according to the frequencies (for imbalanced dataset). The most frequent species would get an index of 0. For a balanced dataset, whichever string appears first will get 0, then so on."]}, {"cell_type": "code", "execution_count": 8, "id": "5bd89a1f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["+------------+-----------+------------+-----------+-----------+-----------------+-------+\n", "|sepal_length|sepal_width|petal_length|petal_width|    species|         features|encoded|\n", "+------------+-----------+------------+-----------+-----------+-----------------+-------+\n", "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|    0.0|\n", "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|    0.0|\n", "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|    0.0|\n", "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|    0.0|\n", "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|    0.0|\n", "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|[5.4,3.9,1.7,0.4]|    0.0|\n", "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|[4.6,3.4,1.4,0.3]|    0.0|\n", "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|[5.0,3.4,1.5,0.2]|    0.0|\n", "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|[4.4,2.9,1.4,0.2]|    0.0|\n", "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|[4.9,3.1,1.5,0.1]|    0.0|\n", "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|[5.4,3.7,1.5,0.2]|    0.0|\n", "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|[4.8,3.4,1.6,0.2]|    0.0|\n", "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|[4.8,3.0,1.4,0.1]|    0.0|\n", "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|[4.3,3.0,1.1,0.1]|    0.0|\n", "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|[5.8,4.0,1.2,0.2]|    0.0|\n", "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|[5.7,4.4,1.5,0.4]|    0.0|\n", "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|[5.4,3.9,1.3,0.4]|    0.0|\n", "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|[5.1,3.5,1.4,0.3]|    0.0|\n", "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|[5.7,3.8,1.7,0.3]|    0.0|\n", "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|[5.1,3.8,1.5,0.3]|    0.0|\n", "+------------+-----------+------------+-----------+-----------+-----------------+-------+\n", "only showing top 20 rows\n", "\n"]}], "source": ["from pyspark.ml.feature import StringIndexer\n", "\n", "labeler = StringIndexer(inputCol=\"species\", outputCol=\"encoded\")\n", "df = labeler.fit(df).transform(df)\n", "df.show()"]}, {"cell_type": "markdown", "id": "866286e1", "metadata": {}, "source": ["The DataFrame now has a new column named `encoded` with new values populated. You can check the new columns have been added to the PySpark DataFrame by creating a new Pandas DataFrame"]}, {"cell_type": "code", "execution_count": 9, "id": "df130f5f", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>sepal_length</th>\n", "      <th>sepal_width</th>\n", "      <th>petal_length</th>\n", "      <th>petal_width</th>\n", "      <th>species</th>\n", "      <th>features</th>\n", "      <th>encoded</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>5.1</td>\n", "      <td>3.5</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>4.9</td>\n", "      <td>3.0</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>4.7</td>\n", "      <td>3.2</td>\n", "      <td>1.3</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>4.6</td>\n", "      <td>3.1</td>\n", "      <td>1.5</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>5.0</td>\n", "      <td>3.6</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>5.4</td>\n", "      <td>3.9</td>\n", "      <td>1.7</td>\n", "      <td>0.4</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[5.4, 3.9, 1.7, 0.4]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>4.6</td>\n", "      <td>3.4</td>\n", "      <td>1.4</td>\n", "      <td>0.3</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.6, 3.4, 1.4, 0.3]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7</th>\n", "      <td>5.0</td>\n", "      <td>3.4</td>\n", "      <td>1.5</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[5.0, 3.4, 1.5, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>4.4</td>\n", "      <td>2.9</td>\n", "      <td>1.4</td>\n", "      <td>0.2</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.4, 2.9, 1.4, 0.2]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>4.9</td>\n", "      <td>3.1</td>\n", "      <td>1.5</td>\n", "      <td>0.1</td>\n", "      <td>Iris-setosa</td>\n", "      <td>[4.9, 3.1, 1.5, 0.1]</td>\n", "      <td>0.0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   sepal_length  sepal_width  petal_length  petal_width      species  \\\n", "0           5.1          3.5           1.4          0.2  Iris-setosa   \n", "1           4.9          3.0           1.4          0.2  Iris-setosa   \n", "2           4.7          3.2           1.3          0.2  Iris-setosa   \n", "3           4.6          3.1           1.5          0.2  Iris-setosa   \n", "4           5.0          3.6           1.4          0.2  Iris-setosa   \n", "5           5.4          3.9           1.7          0.4  Iris-setosa   \n", "6           4.6          3.4           1.4          0.3  Iris-setosa   \n", "7           5.0          3.4           1.5          0.2  Iris-setosa   \n", "8           4.4          2.9           1.4          0.2  Iris-setosa   \n", "9           4.9          3.1           1.5          0.1  Iris-setosa   \n", "\n", "               features  encoded  \n", "0  [5.1, 3.5, 1.4, 0.2]      0.0  \n", "1  [4.9, 3.0, 1.4, 0.2]      0.0  \n", "2  [4.7, 3.2, 1.3, 0.2]      0.0  \n", "3  [4.6, 3.1, 1.5, 0.2]      0.0  \n", "4  [5.0, 3.6, 1.4, 0.2]      0.0  \n", "5  [5.4, 3.9, 1.7, 0.4]      0.0  \n", "6  [4.6, 3.4, 1.4, 0.3]      0.0  \n", "7  [5.0, 3.4, 1.5, 0.2]      0.0  \n", "8  [4.4, 2.9, 1.4, 0.2]      0.0  \n", "9  [4.9, 3.1, 1.5, 0.1]      0.0  "]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["pd.DataFrame(df.take(10), columns=df.columns)"]}, {"cell_type": "markdown", "id": "51da29d5", "metadata": {}, "source": ["Now you have transformed the data as needed. To begin building your model, you need to split the data into a train/test dataset."]}, {"cell_type": "code", "execution_count": 10, "id": "e0debe99", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Train dataset count: 104\n", "Test dataset count: 46\n"]}], "source": ["train, test = df.randomSplit(\n", "    [0.7, 0.3], seed=42\n", ")\n", "print(f\"Train dataset count: {str(train.count())}\")\n", "print(f\"Test dataset count: {str(test.count())}\")"]}, {"cell_type": "markdown", "id": "7dc84832", "metadata": {}, "source": ["Next you will need to instantiate the `RandomForestClassifier` and train the model. At this point before you run the next cell, open up the Spark UI by typing `localhost:4040` into your browser, then navigating to the executors tab."]}, {"cell_type": "code", "execution_count": 11, "id": "b7efe0e3", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.classification import RandomForestClassifier\n", "\n", "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"encoded\")\n", "model = rf.fit(train)\n", "predictions = model.transform(test)"]}, {"cell_type": "markdown", "id": "63a7d1d0", "metadata": {}, "source": ["`featuresCol` is the list of features of the dataframe, which means if you have more features you'd like to include, you could put in a list. You create the model by fitting on the training dataset, then validate it by making predictions on the test dataset. `model.transform(test)` will create new columns, like `rawPrediction`, `prediction`, and `probability`."]}, {"cell_type": "code", "execution_count": 12, "id": "6b30edc8", "metadata": {}, "outputs": [{"data": {"text/plain": ["DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, encoded: double, rawPrediction: vector, prediction: double, probability: vector]"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# if the columns names here are different, do a `printSchema` on top of predictions to see the correct column names\n", "predictions.select(\n", "    \"sepal_length\",\n", "    \"sepal_width\",\n", "    \"petal_length\",\n", "    \"petal_width\",\n", "    \"encoded\",\n", "    \"rawPrediction\",\n", "    \"prediction\",\n", "    \"probability\",\n", ")"]}, {"cell_type": "markdown", "id": "aba54d6f", "metadata": {}, "source": ["You have a trained model, go ahead and evaluate the model by using the `MulticlassClassificationEvaluator`."]}, {"cell_type": "code", "execution_count": 13, "id": "0179edf3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Accuracy: 0.9571428571428571%\n", "Test Error = 0.04285714285714293\n"]}], "source": ["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n", "\n", "evaluator = MulticlassClassificationEvaluator(\n", "    labelCol=\"encoded\", predictionCol=\"prediction\"\n", ")\n", "accuracy = evaluator.evaluate(predictions)\n", "print(f\"Accuracy: {accuracy}%\")\n", "test_error = 1.0 - accuracy\n", "print(f\"Test Error = {test_error}\")"]}, {"cell_type": "markdown", "id": "3c3ef327-719b-41fc-bfd3-b562f9390080", "metadata": {}, "source": ["As you can see, the model performs with 97.8% accuracy and has a test error of 0.021. "]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.17"}, "vscode": {"interpreter": {"hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}}, "nbformat": 4, "nbformat_minor": 5}