{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3c3826",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "In this lab, we would like to a Random Forest Classifier model to study over the ecommerce behavior from a multi-category store. First, we need to download the data to your local machine, then we will load the data from the local machine onto a Pandas Dataframe.\n",
    "\n",
    "### Instruction\n",
    "* Accept the kaggle policy and download the data from here https://www.kaggle.com/code/tshephisho/ecommerce-behaviour-using-xgboost/data\n",
    "* For the first model building, we'll only use the 2019-Nov csv data (which is still around ~2gb zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4425cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession #entry point for pyspark\n",
    "\n",
    "#instantiate spark instance\n",
    "spark = SparkSession.builder.appName('Random Forest eCommerce').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\" #wherever path you saved the kaggle file to\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "df.printSchema() #to see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9475b",
   "metadata": {},
   "source": [
    "We've already used this dataset, but feel free to explore around. Now, we want to use the pandas instead of pyspark, we have to use the `action` functions, which then means there will be a network shuffle. Earlier lab used the Iris dataset which was about ~1KB, but the current dataset may be too large, and may throw an `OutOfMemory` error if we attempt to load the data into the pandas dataframe. I would suggest only take few rows for exploratory analysis if pandas is more comfortable library. Otherwise, sticking with native PySpark functions would be much better option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df.take(10), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008761f",
   "metadata": {},
   "source": [
    "### Know your Customers\n",
    "\n",
    "How many unique customers visit the site?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b67da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using naitve pyspark..\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"user_id\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b20b7b",
   "metadata": {},
   "source": [
    "Did you notice the spark progress bar when you triggered the `action` function? The `show()` function is the `action` function which means the lazy evaluation of Spark was triggered and completed a certain job. `read.csv` should have been another job. If you go to the `localhost:4040` you should be able to see 2 completed jobs under the `Jobs` tab, which are `csv` and `showString`. While a heavy job is getting executed, you can take a look at the `Executors` tab to examine the executors completing the tasks in parellel. Now, we may not see if we run this on a local machine, but this behavior should definitely be visible if you're on a cloud system, such as EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a007f9d",
   "metadata": {},
   "source": [
    "### (Optional) Visitors Daily Trend\n",
    "\n",
    "Does traffic flunctuate by date? Try using the event_time and user_id to see traffic, and draw out the plots for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2038b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try cleaning out the event_time column then using groupby/count\n",
    "# import pyspark.sql.functions as F\n",
    "# use this as a reference to clean the event_time column\n",
    "# https://stackoverflow.com/questions/67827631/how-udf-function-works-in-pyspark-with-dates-as-arguments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192f050",
   "metadata": {},
   "source": [
    "Question: We would still like to see the cart abandonment rate using the dataset. What relevant features can we use for modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae815e1",
   "metadata": {},
   "source": [
    "Now, let's build out the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#columns you'd like to use\n",
    "feature_cols = []\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ad1bc",
   "metadata": {},
   "source": [
    "Is there a labeler column that we'd like to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e68138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#what should we use for the inputCol here?\n",
    "labeler = StringIndexer(inputCol='', outputCol='encoded')\n",
    "df = labeler.fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d44310",
   "metadata": {},
   "source": [
    "Now build the train/test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf826cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='', labelCol='encoded')\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "# what goes in the select() function?\n",
    "predictions.select().show(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a76c9b",
   "metadata": {},
   "source": [
    "Once the job execution is done, try evaluating on how we performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e17ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc19e8",
   "metadata": {},
   "source": [
    "### Extra: Use the confusion matrix to see the other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','encoded']).withColumn('encoded', F.col('encoded').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','encoded'])\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
