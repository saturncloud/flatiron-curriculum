{"cells": [{"cell_type": "markdown", "id": "22a87c5e", "metadata": {}, "source": ["# Random Forest Classifier in PySpark - Lab"]}, {"cell_type": "markdown", "id": "7260babc", "metadata": {}, "source": ["## Introduction  \n", "\n", "In this lab, you will build a Random Forest Classifier model to study the ecommerce behavior of consumers from a multi-category store. First, you will need to download the data to your local machine, then you will load the data from the local machine onto a Pandas Dataframe."]}, {"cell_type": "markdown", "id": "7fe626fb", "metadata": {}, "source": ["## Objectives  \n", "\n", "* Use the kaggle eCommerce dataset in PySpark\n", "* Build and train a random forest classifier in PySpark"]}, {"cell_type": "markdown", "id": "b91f97f0", "metadata": {}, "source": ["## Instruction\n", "* Accept the Kaggle policy and download the data from [Kaggle](https://www.kaggle.com/code/tshephisho/ecommerce-behaviour-using-xgboost/data)\n", "* For the first model you will only use the 2019-Nov csv data (which is still around ~2gb zipped)\n", "* You will run this notebook in a new `pyspark-env` environment following [these setup instructions without docker](https://github.com/learn-co-curriculum/dsc-spark-docker-installation)"]}, {"cell_type": "code", "execution_count": null, "id": "9959b278", "metadata": {}, "outputs": [], "source": ["!pip install pandas"]}, {"cell_type": "code", "execution_count": null, "id": "e7923329-5dbf-4c22-97be-970e4fa0b06c", "metadata": {}, "outputs": [], "source": ["# import necessary libraries\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.dates as dates\n", "from datetime import datetime"]}, {"cell_type": "code", "execution_count": null, "id": "1e52f045-8939-4ce0-a7a3-b3ed04384359", "metadata": {}, "outputs": [], "source": ["from pyspark.sql import SparkSession  # entry point for pyspark\n", "\n", "# instantiate spark instance\n", "spark = (\n", "    SparkSession.builder.appName(\"Random Forest eCommerce\")\n", "    .config(\"spark.executor.memory\", \"4g\")\n", "    .config(\"spark.driver.memory\", \"4g\")\n", "    .master(\"local[*]\")\n", "    .getOrCreate()\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "c1dedf59", "metadata": {}, "outputs": [], "source": ["path = \"../archive/2019-Nov.csv\"  # wherever path you saved the kaggle file to\n", "df = spark.read.csv(path, header=True, inferSchema=True)\n", "df.printSchema()  # to see the schema"]}, {"cell_type": "markdown", "id": "00599c44", "metadata": {}, "source": ["If you want to use Pandas to explore the dataset instead of Pyspark, you have to use the `action` functions, which then means there will be a network shuffle. For smaller dataset such as the Iris dataset which is about ~1KB this is no problem. The current dataset may be too large, and may throw an `OutOfMemory` error if you attempt to load the data into a Pandas dataframe. You should only take a few rows for exploratory analysis if you are more comfortable with Pandas. Otherwise, stick with native PySpark functions. "]}, {"cell_type": "code", "execution_count": 4, "id": "e8dcd29b", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>0</th>\n", "      <th>1</th>\n", "      <th>2</th>\n", "      <th>3</th>\n", "      <th>4</th>\n", "      <th>5</th>\n", "      <th>6</th>\n", "      <th>7</th>\n", "      <th>8</th>\n", "      <th>9</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>event_time</th>\n", "      <td>2019-11-01 00:00:00 UTC</td>\n", "      <td>2019-11-01 00:00:00 UTC</td>\n", "      <td>2019-11-01 00:00:01 UTC</td>\n", "      <td>2019-11-01 00:00:01 UTC</td>\n", "      <td>2019-11-01 00:00:01 UTC</td>\n", "      <td>2019-11-01 00:00:01 UTC</td>\n", "      <td>2019-11-01 00:00:01 UTC</td>\n", "      <td>2019-11-01 00:00:02 UTC</td>\n", "      <td>2019-11-01 00:00:02 UTC</td>\n", "      <td>2019-11-01 00:00:02 UTC</td>\n", "    </tr>\n", "    <tr>\n", "      <th>event_type</th>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "      <td>view</td>\n", "    </tr>\n", "    <tr>\n", "      <th>product_id</th>\n", "      <td>1003461</td>\n", "      <td>5000088</td>\n", "      <td>17302664</td>\n", "      <td>3601530</td>\n", "      <td>1004775</td>\n", "      <td>1306894</td>\n", "      <td>1306421</td>\n", "      <td>15900065</td>\n", "      <td>12708937</td>\n", "      <td>1004258</td>\n", "    </tr>\n", "    <tr>\n", "      <th>category_id</th>\n", "      <td>2053013555631882655</td>\n", "      <td>2053013566100866035</td>\n", "      <td>2053013553853497655</td>\n", "      <td>2053013563810775923</td>\n", "      <td>2053013555631882655</td>\n", "      <td>2053013558920217191</td>\n", "      <td>2053013558920217191</td>\n", "      <td>2053013558190408249</td>\n", "      <td>2053013553559896355</td>\n", "      <td>2053013555631882655</td>\n", "    </tr>\n", "    <tr>\n", "      <th>category_code</th>\n", "      <td>electronics.smartphone</td>\n", "      <td>appliances.sewing_machine</td>\n", "      <td>None</td>\n", "      <td>appliances.kitchen.washer</td>\n", "      <td>electronics.smartphone</td>\n", "      <td>computers.notebook</td>\n", "      <td>computers.notebook</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>electronics.smartphone</td>\n", "    </tr>\n", "    <tr>\n", "      <th>brand</th>\n", "      <td>xiaomi</td>\n", "      <td>janome</td>\n", "      <td>creed</td>\n", "      <td>lg</td>\n", "      <td>xiaomi</td>\n", "      <td>hp</td>\n", "      <td>hp</td>\n", "      <td>rondell</td>\n", "      <td>michelin</td>\n", "      <td>apple</td>\n", "    </tr>\n", "    <tr>\n", "      <th>price</th>\n", "      <td>489.07</td>\n", "      <td>293.65</td>\n", "      <td>28.31</td>\n", "      <td>712.87</td>\n", "      <td>183.27</td>\n", "      <td>360.09</td>\n", "      <td>514.56</td>\n", "      <td>30.86</td>\n", "      <td>72.72</td>\n", "      <td>732.07</td>\n", "    </tr>\n", "    <tr>\n", "      <th>user_id</th>\n", "      <td>520088904</td>\n", "      <td>530496790</td>\n", "      <td>561587266</td>\n", "      <td>518085591</td>\n", "      <td>558856683</td>\n", "      <td>520772685</td>\n", "      <td>514028527</td>\n", "      <td>518574284</td>\n", "      <td>532364121</td>\n", "      <td>532647354</td>\n", "    </tr>\n", "    <tr>\n", "      <th>user_session</th>\n", "      <td>4d3b30da-a5e4-49df-b1a8-ba5943f1dd33</td>\n", "      <td>8e5f4f83-366c-4f70-860e-ca7417414283</td>\n", "      <td>755422e7-9040-477b-9bd2-6a6e8fd97387</td>\n", "      <td>3bfb58cd-7892-48cc-8020-2f17e6de6e7f</td>\n", "      <td>313628f1-68b8-460d-84f6-cec7a8796ef2</td>\n", "      <td>816a59f3-f5ae-4ccd-9b23-82aa8c23d33c</td>\n", "      <td>df8184cc-3694-4549-8c8c-6b5171877376</td>\n", "      <td>5e6ef132-4d7c-4730-8c7f-85aa4082588f</td>\n", "      <td>0a899268-31eb-46de-898d-09b2da950b24</td>\n", "      <td>d2d3d2c6-631d-489e-9fb5-06f340b85be0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                                  0  \\\n", "event_time                  2019-11-01 00:00:00 UTC   \n", "event_type                                     view   \n", "product_id                                  1003461   \n", "category_id                     2053013555631882655   \n", "category_code                electronics.smartphone   \n", "brand                                        xiaomi   \n", "price                                        489.07   \n", "user_id                                   520088904   \n", "user_session   4d3b30da-a5e4-49df-b1a8-ba5943f1dd33   \n", "\n", "                                                  1  \\\n", "event_time                  2019-11-01 00:00:00 UTC   \n", "event_type                                     view   \n", "product_id                                  5000088   \n", "category_id                     2053013566100866035   \n", "category_code             appliances.sewing_machine   \n", "brand                                        janome   \n", "price                                        293.65   \n", "user_id                                   530496790   \n", "user_session   8e5f4f83-366c-4f70-860e-ca7417414283   \n", "\n", "                                                  2  \\\n", "event_time                  2019-11-01 00:00:01 UTC   \n", "event_type                                     view   \n", "product_id                                 17302664   \n", "category_id                     2053013553853497655   \n", "category_code                                  None   \n", "brand                                         creed   \n", "price                                         28.31   \n", "user_id                                   561587266   \n", "user_session   755422e7-9040-477b-9bd2-6a6e8fd97387   \n", "\n", "                                                  3  \\\n", "event_time                  2019-11-01 00:00:01 UTC   \n", "event_type                                     view   \n", "product_id                                  3601530   \n", "category_id                     2053013563810775923   \n", "category_code             appliances.kitchen.washer   \n", "brand                                            lg   \n", "price                                        712.87   \n", "user_id                                   518085591   \n", "user_session   3bfb58cd-7892-48cc-8020-2f17e6de6e7f   \n", "\n", "                                                  4  \\\n", "event_time                  2019-11-01 00:00:01 UTC   \n", "event_type                                     view   \n", "product_id                                  1004775   \n", "category_id                     2053013555631882655   \n", "category_code                electronics.smartphone   \n", "brand                                        xiaomi   \n", "price                                        183.27   \n", "user_id                                   558856683   \n", "user_session   313628f1-68b8-460d-84f6-cec7a8796ef2   \n", "\n", "                                                  5  \\\n", "event_time                  2019-11-01 00:00:01 UTC   \n", "event_type                                     view   \n", "product_id                                  1306894   \n", "category_id                     2053013558920217191   \n", "category_code                    computers.notebook   \n", "brand                                            hp   \n", "price                                        360.09   \n", "user_id                                   520772685   \n", "user_session   816a59f3-f5ae-4ccd-9b23-82aa8c23d33c   \n", "\n", "                                                  6  \\\n", "event_time                  2019-11-01 00:00:01 UTC   \n", "event_type                                     view   \n", "product_id                                  1306421   \n", "category_id                     2053013558920217191   \n", "category_code                    computers.notebook   \n", "brand                                            hp   \n", "price                                        514.56   \n", "user_id                                   514028527   \n", "user_session   df8184cc-3694-4549-8c8c-6b5171877376   \n", "\n", "                                                  7  \\\n", "event_time                  2019-11-01 00:00:02 UTC   \n", "event_type                                     view   \n", "product_id                                 15900065   \n", "category_id                     2053013558190408249   \n", "category_code                                  None   \n", "brand                                       rondell   \n", "price                                         30.86   \n", "user_id                                   518574284   \n", "user_session   5e6ef132-4d7c-4730-8c7f-85aa4082588f   \n", "\n", "                                                  8  \\\n", "event_time                  2019-11-01 00:00:02 UTC   \n", "event_type                                     view   \n", "product_id                                 12708937   \n", "category_id                     2053013553559896355   \n", "category_code                                  None   \n", "brand                                      michelin   \n", "price                                         72.72   \n", "user_id                                   532364121   \n", "user_session   0a899268-31eb-46de-898d-09b2da950b24   \n", "\n", "                                                  9  \n", "event_time                  2019-11-01 00:00:02 UTC  \n", "event_type                                     view  \n", "product_id                                  1004258  \n", "category_id                     2053013555631882655  \n", "category_code                electronics.smartphone  \n", "brand                                         apple  \n", "price                                        732.07  \n", "user_id                                   532647354  \n", "user_session   d2d3d2c6-631d-489e-9fb5-06f340b85be0  "]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["pd.DataFrame(df.take(10), columns=df.columns).transpose()"]}, {"cell_type": "markdown", "id": "7a4be40f", "metadata": {}, "source": ["### Know your Customers\n", "\n", "How many unique customers visit the site?"]}, {"cell_type": "code", "execution_count": 5, "id": "8b065821", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["                                                                                \r"]}, {"name": "stdout", "output_type": "stream", "text": ["+-----------------------+\n", "|count(DISTINCT user_id)|\n", "+-----------------------+\n", "|                3696117|\n", "+-----------------------+\n", "\n"]}], "source": ["# using native pyspark\n", "from pyspark.sql.functions import countDistinct\n", "\n", "df.select(countDistinct(\"user_id\")).show()"]}, {"cell_type": "markdown", "id": "b5d3802c", "metadata": {}, "source": ["Did you notice the spark progress bar when you triggered the `action` function? The `show()` function is the `action` function which means the lazy evaluation of Spark was triggered and completed a certain job. `read.csv` should have been another job. If you go to `localhost:4040` you should be able to see 2 completed jobs under the `Jobs` tab, which are `csv` and `showString`. While a heavy job is getting executed, you can take a look at the `Executors` tab to examine the executors completing the tasks in parellel. Now, you may not see if we run this on a local machine, but this behavior should definitely be visible if you're on a cloud system, such as EMR."]}, {"cell_type": "markdown", "id": "76339535", "metadata": {}, "source": ["### (Optional) Visitors Daily Trend\n", "\n", "Does traffic flunctuate by date? Try using the event_time to see traffic, and draw the plots for visualization."]}, {"cell_type": "code", "execution_count": null, "id": "998938b5-1b63-45c2-ba6a-dbbd6b05c7af", "metadata": {}, "outputs": [], "source": ["# for event_time you should use a window and groupby a time period\n", "from pyspark.sql.functions import window"]}, {"cell_type": "markdown", "id": "352dadc9", "metadata": {}, "source": ["Question: You would still like to see the cart abandonment rate using the dataset. What relevant features can we use for modeling?"]}, {"cell_type": "code", "execution_count": null, "id": "2e26f8f9", "metadata": {}, "outputs": [], "source": ["# your answer"]}, {"cell_type": "markdown", "id": "bdeea5b9", "metadata": {}, "source": ["Now, you will start building the model. Add the columns you would like to use for predictor features in the model to the `feature_cols` list"]}, {"cell_type": "code", "execution_count": null, "id": "f55df7c5", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.feature import VectorAssembler\n", "\n", "feature_cols = []  # columns you'd like to use\n", "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n", "df = assembler.transform(df)\n", "df.show()"]}, {"cell_type": "markdown", "id": "1d0c53a7", "metadata": {}, "source": ["To use a string column, you can use the `StringIndexer` to encode the column. Update the `inputCol` keyword argument so that you can encode the target feature."]}, {"cell_type": "code", "execution_count": null, "id": "44d5fce1", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.feature import StringIndexer\n", "\n", "labeler = StringIndexer(\n", "    inputCol=\"\", outputCol=\"encoded\"\n", ")  # what should we use for the inputCol here?\n", "df = labeler.fit(df).transform(df)\n", "df.show()"]}, {"cell_type": "markdown", "id": "ecacb771", "metadata": {}, "source": ["Now build the train/test dataset with a 70/30 `randomSplit` and a random seed set to 42"]}, {"cell_type": "code", "execution_count": null, "id": "d8be79a2", "metadata": {}, "outputs": [], "source": ["train, test = df.randomSplit()\n", "print(\"Training Dataset Count: \" + str(train.count()))\n", "print(\"Test Dataset Count: \" + str(test.count()))"]}, {"cell_type": "markdown", "id": "ea4d4f7a-580d-4890-af7a-47675b158d3a", "metadata": {}, "source": ["Next you need to add in the name of the feature column and the name of the `labelCol` you previously encoded for training the model."]}, {"cell_type": "code", "execution_count": null, "id": "be3d1b71", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.classification import RandomForestClassifier\n", "\n", "rf = RandomForestClassifier(featuresCol=\"\", labelCol=\"\")\n", "model = rf.fit(train)\n", "predictions = model.transform(test)\n", "# what goes in the select() function?\n", "predictions.select().show(25)"]}, {"cell_type": "markdown", "id": "9fdadc1f", "metadata": {}, "source": ["Once the job execution is done, evaluate the model's performance. Add in the `labelCol` below."]}, {"cell_type": "code", "execution_count": null, "id": "e94817fb", "metadata": {}, "outputs": [], "source": ["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n", "\n", "evaluator = MulticlassClassificationEvaluator(labelCol=\"\", predictionCol=\"prediction\")\n", "accuracy = evaluator.evaluate(predictions)\n", "print(\"Accuracy = %s\" % (accuracy))\n", "print(\"Test Error = %s\" % (1.0 - accuracy))"]}, {"cell_type": "markdown", "id": "7a99d0a4", "metadata": {}, "source": ["### Extra: Use the confusion matrix to see the other metrics"]}, {"cell_type": "code", "execution_count": null, "id": "e22d2830", "metadata": {}, "outputs": [], "source": ["from pyspark.mllib.evaluation import MulticlassMetrics\n", "from pyspark.sql.types import FloatType\n", "import pyspark.sql.functions as F\n", "\n", "preds_and_labels = (\n", "    predictions.select([\"prediction\", \"encoded\"])\n", "    .withColumn(\"encoded\", F.col(\"encoded\").cast(FloatType()))\n", "    .orderBy(\"prediction\")\n", ")\n", "preds_and_labels = preds_and_labels.select([\"prediction\", \"encoded\"])\n", "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n", "print(metrics.confusionMatrix().toArray())"]}, {"cell_type": "code", "execution_count": null, "id": "6d042f7e-d5da-4c3e-afd1-5935929fef27", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.17"}}, "nbformat": 4, "nbformat_minor": 5}